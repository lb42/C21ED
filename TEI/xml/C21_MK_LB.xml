<?xml version="1.0" encoding="UTF-8"?><TEI xmlns="http://www.tei-c.org/ns/1.0"><teiHeader><fileDesc><titleStmt><title>C21 Editions:  Lou Burnard</title><title type="sub">Interview conducted by  Michael Kurzmeier		 on Date: June 2nd, 2022</title></titleStmt><publicationStmt><p>Unpublished working draft</p></publicationStmt><sourceDesc><bibl> O'Sullivan, J., M. Pidd, M. Kurzmeier, O. Murphy, and B. Wessels.
                            (2023). <title>Interviews about the future of scholarly digital editions
                                [Data files].</title> Available from <ref target="https://www.dhi.ac.uk/data/c21editions">https://www.dhi.ac.uk/data/c21editions</ref> (File C21_MK_LB.xml downloaded: 8th June 2023).</bibl></sourceDesc></fileDesc><encodingDesc><p>Retagged in TEI P5 from RTF (via soffice and docxtotei) </p></encodingDesc><revisionDesc><listChange><change><date>$today</date>Header added</change></listChange></revisionDesc></teiHeader><text><front><div><head>Interviewee bio</head><p>LOU BURNARD was assistant director of Oxford University Computing Services (OUCS) from 2001 to September 2010 when he took early retirement. Prior to that, he was manager of the Humanities Computing Unit at OUCS for five years. He has worked in ICT support for research in the humanities since the 1990s. He was one of the founding editors of the Text Encoding Initiative (TEI) and continues to play an active part in its maintenance and development. He has played a key role in the establishment of many other key activities and initiatives in this area, such as the UK Arts and Humanities Data Service, and the British National Corpus and has published and lectured widely. Between 2008 and 2011 he was a Member of the Conseil Scientifique for the CNRS-funded “Adonis” TGE.</p></div></front><body><sp who="#MK"><speaker>MK</speaker><p>	So, I want to talk about the terms digital scholarly editing and digital publishing, and I’d like to know, in the broadest sense, what do those terms mean to you.</p></sp><sp who="#LB"><speaker>LB</speaker><p>	Ok. So, first of all, there are two of them; digital scholarly editing and digital publishing, right?</p></sp><sp who="#MK"><speaker>MK</speaker><p>	Right.</p></sp><sp who="#LB"><speaker>LB</speaker><p>	I ask the question because - in French you use the same word for editing and for publishing, so I’m not sure about the distinction. You can edit but if nobody ever sees it it’s not published and you can publish things without actually having edited them, in the sense that most people use the word, I guess. And I think that ambiguity can carry over perfectly neatly into the digital world. Sticking the prefix digital in front of them, really doesn’t change anything about the intent of the word at least. I think it’s probably true that some fixed phrases like “digital publishing” become reified, there are so many people around who say ‘oh, I do digital editing as opposed to I’m an editor', The addition of the word digital into the mix is really a way of valorising it and saying that this is somehow more up to date, better, etc. The question I would have is, can you edit without using a computer anyway? People have been using word processors ever since word processors became available. I don’t know whether you know the seminal article by Allen Renear and company about the future of scholarly publishing which appeared in the Journal of the ACM, a very prestigious computer science venue back in (God knows when) sometime before you were born <!--[1988]--> and in which they bemoaned the fact that as people more and more start using word processors, they will somehow lose touch with the essentials of scholarly thinking/scholarly editing. It’s a very interesting piece of polemic both because of what happened afterwards which was, as we all know the emergence of the TEI and so on but also because of the way in which these bright brash young graduate students at Brown in the states are unconsciously reprising debates which took place in 19th Century England about the rise of the typewriter. Technology has always been there it’s just that people suddenly realise they can use that technology to do what they have always done but in a quicker and more convenient way. That’s all that digital editing means to me I’m afraid. The latest gizmo is being applied to a very, very old tradition.</p></sp><sp who="#MK"><speaker>MK</speaker><p>	It’s interesting to see those debates about the integration of technology into those fields and how they seem to run in cycles and eventually you seem to hit the beginning of a new cycle and the end of the previous cycle. But generally, because you’ve had this long career and much of it has been dedicated to research and initiatives that are of relevance to digital humanities of course so I would like to ask you throughout that time what observations have you made in terms of the dominant ways in which scholars approach digital editing? Have you noticed any changes there, what do you think of how scholars approach the whole topic of digital editing?</p></sp><sp who="#LB"><speaker>LB</speaker><p>	I would like to say that there has been progress on a very, very specific, perhaps too much from my own point of view a front, a battle front if you like. I think there’s always been a bit of a problem in this field, however you want to define the field; there’s always been a tension between the scholar who really wants to focus on some brilliant literary or linguistic insight and the person who’s actually going to get their hands dirty on the keyboard. In France there is quite a sharp distinction that runs all the way through academia, all the way through the promotion system and the reward system between the researcher and the engineer, as they are called. The job that I have always done has been to try to bridge that gap; to try to get the scholar to understand the engineering and the engineer to understand the scholarship. It didn’t use to be like this. For better or worse, you know, hundreds of years ago the people who used computers to do scholarly editing, (though they didn’t call it that), had to write the code themselves. I always like to cite the case of a Shakespearean scholar called Trevor Howard-Hill, sadly now deceased. He produced some of the first published concordances to Shakespeare quartos. His scholarly work was interesting and unusual. Particularly so because it was published in traditional book form, – he actually managed to persuade OUP <!--[Oxford University Press]--> to publish a whole series of concordances to individual quarto plays - but the most remarkable thing about all this is that he had to write the code to do the job, to do the concordancing, to do the indexing himself and he had to make up the coding conventions himself. But he was a traditional, graduate student studying what became Book History: he was interested in identifying the compositors who set the first folio, that was the subject of his thesis. And there's a big jump from that situation to today’s scholar who might say, ‘oh I need this text encoded, go and do it’ to one of his research assistants. Maybe you’ve been that research assistant, I don’t know? And we see this in all sorts of ways. One of the things that really gets up my nose is the tendency of some scholars to carry our their research using farms of graduate students working on particular difficult, minute, technical tasks. The students do all the work and then the academic writes the article that makes a synthesis of the results and gets all the credit. I don’t think that’s right. I think there should be more co-operation. I think more scholars need to be willing to embrace the technology and this is particularly true for text encoding which has enormous benefits. If you want to understand a text, really, there is no better way than to try to encode it, in my opinion. I am not the only one to have this opinion so I think I can assert it as a genuine fact! I think we are beginning to see some steps in that direction which are encouraging to say the least.</p></sp><sp who="#MK"><speaker>MK</speaker><p>	Thank you. Following on that and the examples that you were just talking about, have you seen any particularly unusual approaches to digital editing that have stuck in your mind over time?</p></sp><sp who="#LB"><speaker>LB</speaker><p>	One of the first that really impressed me recently, well it’s not very recent anymore, was Dirk van Hulle’s edition of the Samuel Beckett Manuscripts . I thought that was really impressive because of the way that it managed and made visible the immense fluidity and complexity of Beckett’s manuscript archives. I guess that would be pretty high up on my list.</p><p>I think one of the things that we’re missing from this conversation is a clear sense of what we actually mean when we’re talking about digital editions. A few years ago, I flew a kite about the fact that we didn’t really need any more digital editions: what we needed was digital non-editions, what we needed was un-mediated, access to primary source materials in so far as that is possible and that we should reward people more for making things like digital transcriptions of works that are otherwise inaccessible. That didn’t seem to take off for various reasons. </p><p>I’ll give you an example, if I may, from what I’m currently doing, and which is one of the reasons I was in Galway. There’s a very interesting kind of a corpus of 19th Century dramatic texts called Lacy’s Acting Edition. Thomas Hailes Lacy was a very enterprising Victorian publisher and dramatist and actor but he’s remembered now for the fact that roughly every year from about 1840 to 1880, he published a selection of a dozen or so popular dramas, aimed at acting professionals and amateurs alike. When he retired and sold the business to Samuel French, an American with whom he’d worked before, the series contained upwards of 1500 titles and can, I suggest be considered a kind of approximation to a canon, of 19th Century drama. I’m interested in exploring that notion because I’m interested in exploring the notion of canon, but I’m also interested in how you get access to these plays and work with them in a productive way, using encoding techniques, using text analysis and so on. if you go to your library, they will probably have a copy of one or two of these volumes; if you go and look at the big national collections they certainly will have digitised page images of some of them. You could collect all of those digitised page images but as you know, working with digitised page images in PDF is not at all easy if what you are interested in is, shall we say to take a random example, how often are female speakers centre stage and how often are male speakers centre stage? Forget it, you want to do that with a bunch of PDFs, no way. It’s really depressing to think that in the evolution and establishment of the digital canon what we’re seeing for purely pragmatic reasons of cost is just more and more PDF, just more and more images of pages. The technology for analysing images of pages advances tremendously and you can do great stuff and I don’t wish to decry it and thank God that it is available because otherwise there would be nothing, nothing at all! And it’s clear that the availability of these digital collections from our national libraries has enormously changed the way scholarship takes place. You can easily read these plays which is terrific, you don’t have to have a travel grant anymore and so on. But how much more you could do if only there was, shall we say, a regular path for converting the PDF files into something more easily, more productively analysed, shall we say. So, I’m interested in that aspect of alternative canons, if you like. I’ve forgotten what the question was, but I hope that was a good answer.</p></sp><sp who="#MK"><speaker>MK</speaker><p>	It was a good answer. No, perfect, because it leads me to my next question here, I would like to talk a little bit about tools and platforms and you were already mentioning file formats and limitations and affordances of them, I would like to dig a little deeper there in, first of all, what do you see are the dominant tools and platforms in the broadest sense and what do you see as their affordances and limitations, kind of building on to what we were talking about with different file formats.</p></sp><sp who="#LB"><speaker>LB</speaker><p>	Ok. I have no shame in saying this… my dear late friend and mentor and all-round wonderful guy Sebastian Rahtz always used to begin presentations in a place where he wasn’t known by saying, ‘I’m Sebastian Rahtz, I’m an open-source bigot’. I feel the same. I’m an open-source bigot. I’m not interested in fancy software that requires me to purchase a licence and pay through the nose forever to use it. I’m not interested in wonderful analytic systems that require me to purchase such software. I don’t care. Call me irrational, call me illogical, call me an old hippie, I don’t care but the fact of the matter is we advance by sharing, we don’t advance by locking things away and closing things up. This is such an obvious point; I’m astonished that in the year of grace 2022 I still have to talk about it. Anyway, to get more practical the specific tools that I use every day are all the ones you’d expect: Oxygen XML Editor, XSLT, Saxon, whatever, I do a lot of work doing text encoding and those are the natural tools for me to use. When it comes to analysis, well, this is another penalty or privilege one or the other of being old: I’ve seen programming languages come and go <stage>laughs</stage> I feel sometimes like a cab driver, ‘that Fortran I had him in the back of my car’, I’ve done Fortran, I’ve done ALGOL I’ve done Perl I’ve done SNOBOL not many people know that. I’ve just noticed that the kids on the block are all using Python so I’m learning Python and then I discover actually no there’s something else around the corner that everybody wants to use ... I don’t know. They’re all the same all of these languages fundamentally, or at least there are more similarities than differences when it comes to… you used the nice word ‘affordances’ so I’ll use that too. I like systems where the affordances give you access to the abstract model that you’ve worked on in your encoding, does that make sense?</p></sp><sp who="#MK"><speaker>MK</speaker><p>	Absolutely.</p></sp><sp who="#LB"><speaker>LB</speaker><p>	I am really tired of analytic systems that take a beautifully encoded text and say right throw away all the tags and we’ll just look at the strings of what’s left, I mean, you know, Wordsworth had a term for that. <stage>laughs</stage> I don’t know. Are you looking for specific recommendations here or what?</p></sp><sp who="#MK"><speaker>MK</speaker><p>	No, no, I’m perfectly happy with that open-source praise.</p></sp><sp who="#LB"><speaker>LB</speaker><p>	So yeah, this is another ongoing gripe… I’ve been doing quite a bit of work with people who are interested in distant reading and linguistic analysis. There is a whole raft of fantastically good software that you can use for analysing automatically the linguistic properties of texts, morphological aspects, syntactic properties, all of that stuff, really interesting, the only problem is it only operates on pure text. Now, if you’ve got what I would consider a properly encoded text, and you want to enrich that encoding with linguistic analysis, it seems that you have to throw away all of your markup first, get the linguistic analysis done, re-express that in XML or whatever and then somehow have a way of reintegrating with the mark up that you just threw away That’s not a clear or very satisfactory work flow. So, that’s a good example of a problem caused by the fragmentation of scholarly disciplines The people who are interested in the texts as texts, the philological stream, let’s put it like that, clearly see the benefit of putting in TEI XML Markup and the people who are doing linguistic analysis clearly don’t. And this is a problem we have had ever since the beginning of TEI actually.</p></sp><sp who="#MK"><speaker>MK</speaker><p>	Yes, and building on that I’d like to know from you, is there something more generally you think we’re missing in the field of digital publishing and editions and resources. We talked about fragmentation and maybe we’re talking about a fascination with tools, that might be a general DH condition.</p></sp><sp who="#LB"><speaker>LB</speaker><p>	I don’t perceive any obvious gaps other than in the areas that we’ve been talking about. But I can’t honestly say I have my finger on the pulse of what’s happening here more generally. I’m retired, I don’t go into an office every day, I don’t supervise students or anything. Just for fun I keep myself active looking at particular projects that interest me, so my view of the whole field is pretty limited.</p></sp><sp who="#MK"><speaker>MK</speaker><p>	Ok then let’s move on. I want to talk a little bit about what you have done and obviously you were involved in the Oxford Text Archive and then I’d generally like to know what do you think are the most significant projects in your career that you have worked on?</p></sp><sp who="#LB"><speaker>LB</speaker><p>	Personally, I would say that although setting up the Oxford Text Archive was fun at the time, I wasn’t the only person who saw the need to archive digital materials, even in the early days. And nowadays digital archiving is properly seen as the job of libraries; it is the job of public archives to look after texts however they are published. The only reason I set up the Oxford Text Archive is because if I’d gone to the Bodleian at the time and said ‘look, don’t you think you should be collecting this stuff’ they’d have laughed. So, I wouldn’t put that down. I would much prefer to be associated with the origins of the Text Encoding Initiative, that was one of the biggest intellectual adventures of my life. And also, the British National Corpus, I don’t know if that shows up on whatever it is you looked me up on, but the British National Corpus was one of the pioneering projects in corpus linguistics and I was proud to have been involved closely with that.</p></sp><sp who="#MK"><speaker>MK</speaker><p>	Ok, thank you. And talking about these projects is there anything that you would have liked to have done differently looking back now.</p></sp><sp who="#LB"><speaker>LB</speaker><p>	I would have liked to have been better paid.</p></sp><sp who="#MK"><speaker>MK</speaker><p>	Sure.</p></sp><sp who="#LB"><speaker>LB</speaker><p>	<stage>laughs</stage> I think, this is a personal thing really, something I would have done differently? I should have been nicer to people. During my career developed a reputation for being a little acerbic. This dawned on me when a very nice American scholar came up to me in the lunch queue after we’d talked about something else and said, ‘oh, you’re not at all as unfriendly as I’ve been told!’ And actually, if I wanted to be pretentious about this, I would say there is something odd about the way in which a person’s persona develops independently of the person’s volition or action even. And I think one of the consequences of the fact that we can do what we are doing currently, you know, having a conversation where you can see me and I can see you although we are hundreds of miles apart and blah, blah, blah. One of the consequences of this is that the persona gets more and more simplified, caricatured even, independent of the person, yeah. I mean, you’re forever going to associate the persona of Lou Burnard with this lovely view that you have of me sitting in my garden, which becomes so vivid and dominant it blots out a more nuanced perception. You don’t have anything like that detailed coverage of many other aspects of my life and you never will. This is why I think the development of persona rather than person is something that digital technology exacerbates. Chew on that.</p></sp><sp who="#MK"><speaker>MK</speaker><p>	I will chew on that. That was an interesting side line, I’ll think about that. But for now, let’s come back and talk about the TEI. Could you walk me through the original motivations of setting up the TEI?</p></sp><sp who="#LB"><speaker>LB</speaker><p>	Probably not. The TEI was set up in 1987. I’ve written a brief article in which I’ve said everything I think about it published in the TEI Journal. But, basically, the people who started the TEI were leading lights in what was not yet known as the Digital Humanities Community and they were for the most part and I say this with no disrespect, they were outliers in their traditional scholarly fields who had discovered that there was something interesting to be done with computers, who had subsequently discovered that the production of the raw material of scholarship, texts, was kind of crazy and non-standardised and difficult to harmonise. The whole purpose of scholarship is to have a uniform access to a huge variety of materials: that’s why we have libraries. But there was nothing analogous for material in digital form. Not only that but there were active pressures, I’m talking 1987 now, there were active pressures to develop different standards for different commercial purposes to get commercial edge. You’ve got to remember in 1987 the only way you could play a CD-ROM was to have the right version of Windows. So, there was a lot of interest there and a lot of pressure and a lot of interesting people met together, and it was also and this is relevant to the point I was making earlier about frontiers and disciplinary divides, the TEI brought together a very mixed bunch of people. There were literary people but there were linguistics people and there were computer science people and there were people doing history and it was an unusual meeting because a lot of people didn’t understand each other, frankly, or they did understand and they suddenly saw the potential for synergy and that was one of the most exciting aspects of the whole thing, I think.</p></sp><sp who="#MK"><speaker>MK</speaker><p>	Thank you and let’s drill down a little bit on adaptation of TEI.</p></sp><sp who="#LB"><speaker>LB</speaker><p>	Adaptation?</p></sp><sp who="#MK"><speaker>MK</speaker><p>	Adoption, so how do you see TEI being adopted and why do you think people use it and why do you think people do not like to use it.</p></sp><sp who="#LB"><speaker>LB</speaker><p>	Somebody has written an article on when not to use the TEI, John Lavagnino, and I read the article when it came out years ago and thought actually, he’s very unpersuasive. As a matter of fact, there are no occasions on which you would not use the TEI so obviously he was having trouble finding them.</p></sp><sp who="#MK"><speaker>MK</speaker><p>	Can you explain that?</p></sp><sp who="#LB"><speaker>LB</speaker><p>	For anything involving text, anything where you want to say something about the reading of a linguistically created artifact of some sort, TEI is the best answer, Sure, you can transcribe everything and put it into a relational database but as soon as you start trying to do anything textual with it, you hit all sorts of limitations. I did my time with database work, so, I know. <stage>laughs</stage></p><p>There is a lot of argument about whether the TEI is good at handling general networks in a way that you know web standards suggest and there have been proposals for bridging that gap. I think the upshot, the bottom line for me is that, yes, you can do really cool things analysing networks using specialised network analysis software but -- it’s a bit like the problem with the linguists, – you are required to use a particular kind of software with a fixed set of capabilities. You lose the general applicability which is one of the strengths of the TEI.</p></sp><sp who="#MK"><speaker>MK</speaker><p>	What I’d like to talk a little bit here is critique for the TEI and I’ve pulled out some critique by Peter Robinson saying basically describing the TEI as being reductionist and arguing that it’s based on too simplistic a notion of what text really is while neglecting its materiality, I would like to hear your response to something like that.</p></sp><sp who="#LB"><speaker>LB</speaker><p>	Well, yes, it is true that the TEI places more emphasis on immaterial aspects of texts than Peter would like, however, there are people who use the TEI to analyse texts with a huge degree of attention to their materiality, so it is simply false to assert that the TEI does not support that kind of work, it does. Whether it’s a good idea or not is another question. So, my response would simply be ‘bollox’.</p></sp><sp who="#MK"><speaker>MK</speaker><p>	That would be a good quote.</p></sp><sp who="#LB"><speaker>LB</speaker><p>	I’m sure you could find a more acceptable way of quoting that.</p></sp><sp who="#MK"><speaker>MK</speaker><p>	No, no, it’s perfect just like that. Let’s talk about the semantic value of text markup. In a poem for example what’s the critical value of marking up all the lines in the poem with the line tag just for example, is there really something that we add to the text by doing that?</p></sp><sp who="#LB"><speaker>LB</speaker><p>	Ok, this is a more interesting question Another critic of the TEI, who also worked very closely with the TEI for a while, Manfred Thaller, uses a nice phrase “markup voodoo” which is the idea that if something moves you should tag it immediately, if you spot something, tag it, no matter whether or not you have any use for it and I think he’s quite right to question that. The purpose of marking up a text is to give it those affordances which a pure image of the text or a transcript of the words of the text alone would lose. So, in the case that you mentioned, verse lines, if you are never going to be interested in metrical properties of a poem, there’s no point in marking up the verse lines. If you’re never going to want to print it out so that it looks like a poem, ditto, however, neither of those situations seems to be the case for most people. I’ve been confronted by this issue because of the work I’ve been doing with the ELTeC project.</p></sp><sp who="#MK"><speaker>MK</speaker><p>	Can you explain?</p></sp><sp who="#LB"><speaker>LB</speaker><p>	There’s a European COST Action called Distant Reading for Literary Studies and one of the things that it’s produced is called the ELTeC which is the European Literary Text Collection This is a collection of corpora, each containing a hundred novels from the 19th Century for each of about 15 European languages, with all sorts of interesting methodological issues. But the one that’s relevant to our conversation now has to do with the degree of markup that it’s practical to introduce when you are talking about that sort of scale of operation and in particular when you’re talking about marking up texts that come from, shall we say, quite different traditions even if they are also very similar in some respects. The novel is a form that is very widespread during the 19th Century across Europe, and while there are some conventions that are exactly the same there are many others that are not. So, proposing a minimal set of encoding, things that must be distinguished in every ELTeC corpus was quite an interesting challenge. We wound up with really a very small set of things that we felt that if you didn’t mark these things up then you would not be able to do useful comparison amongst the components of the corpus. Let me give an example of a boundary case where people could have jumped either way. Novels very frequently contain embedded quotations; you get a little quote at the beginning of a chapter, and you get songs and what have you. Where these embedded things were in verse we agreed that it was necessary to show that they were in verse because verse has different linguistic properties from prose, but we did not agree that it would be essential to show that this little bit of verse embedded in the middle of a chapter was a quotation. So, some people did it and some people didn’t and that’s why I say it’s an interesting boundary case.</p></sp><sp who="#MK"><speaker>MK</speaker><p>	So, what would an ideal future digital edition look like to you, in the broadest sense, is there anything that comes to mind?</p></sp><sp who="#LB"><speaker>LB</speaker><p>	I think the word I would use is context. I think that a traditional print edition comes with all sorts of trammels, all sorts of things attached to it. Some are unstated, to do with the status of the editor and the status of the publisher and the way the edition is presented. But they are still important. If it smells strongly of scholarship in the old-fashioned sense, with footnotes on the page, big type for the text and so on, clear indication and analysis of the variability amongst other versions of the text, all those affordances that tell you this is a well-established, critically created, intellectually respectable production. What is the equivalent in the digital arena? And I suggest that is hypertext, in the good old-fashioned sense. Links which enable me when I read this text to have access to the context in which this source was created, the history of how it has been treated over time, a huge amount of what librarians call metadata associated with it that. Early hypertext theoreticians thought that hypertext was ‘woah’ great way of getting lost in the jungle didn’t they and there’s all sorts of great imagery of endless forking paths taken from Borges and so on and that’s what a digital edition should be if you take that perspective. That’s not what I’m talking about though, I think what the infinite connectability tells us is, that we need to find ways of presenting this supporting material, we need to find ways of locating individual texts within their historical context, within their reception context. I’m trying to do that with my work on Lacy’s plays too because they have a very interesting transmission history, for example. So, is that kind of an answer?</p></sp><sp who="#MK"><speaker>MK</speaker><p>	Perfect, thank you so much. Lou, before we wrap this up is there anything else that you feel like needs to be said on record and hasn’t been said yet.</p></sp><sp who="#LB"><speaker>LB</speaker><p>	Well, I’m sure there is but I don’t know what it is.</p></sp></body></text></TEI>