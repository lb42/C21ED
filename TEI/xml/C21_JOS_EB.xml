<?xml version="1.0" encoding="UTF-8"?><TEI xmlns="http://www.tei-c.org/ns/1.0"><teiHeader><fileDesc><titleStmt><title>C21 Editions:  Elli Bleeker</title><title type="sub">Interview conducted by  James O’Sullivan		 on January 13th, 2022</title></titleStmt><publicationStmt><p>Unpublished working draft</p></publicationStmt><sourceDesc><bibl> O'Sullivan, J., M. Pidd, M. Kurzmeier, O. Murphy, and B. Wessels.
                            (2023). <title>Interviews about the future of scholarly digital editions
                                [Data files].</title> Available from <ref target="https://www.dhi.ac.uk/data/c21editions">https://www.dhi.ac.uk/data/c21editions</ref> (File C21_JOS_EB.xml downloaded: 8th June 2023).</bibl></sourceDesc></fileDesc><encodingDesc><p>Retagged in TEI P5 from RTF (via soffice and docxtotei) </p></encodingDesc><revisionDesc><listChange><change><date>$today</date>Header added</change></listChange></revisionDesc></teiHeader><text><front><div><head>Interviewee bio</head><p>ELLI BLEEKER works as a researcher at the Huygens Institute for the History of the Netherlands. As a Research Fellow in the Marie Sklodowska-Curie funded network DiXiT (2013–2017), she received advanced training in manuscript studies, text modeling, and XML technologies for text modeling. She completed her PhD at the Centre for Manuscript Genetics at Antwerp University (2017) on the role of the scholarly editor in the digital environment. She specialized in digital scholarly editing with a focus on modern manuscripts, genetic criticism, and semi-automated collation. Currently, she works together with Ronald Haentjens Dekker and studies the potential of graph technologies for the modeling of literary and historical texts. This confronts her frequently with complex manuscripts that are very challenging to model computationally. Still, she would choose it again without a doubt. And, to enthuse others for the research field of (digital) scholarly editing, she set up the Companion project together with colleagues Marijke van Faassen, Rik Hoekstra, and Marijn Koolen. She is associate editor of the journal Variants, board member of the European Society for Textual Scholarship and the DHBenelux Steering Committee, and she recently became a member of the Dutch Society for Textual Scholarship.</p></div></front><body><sp who="#JOS"><speaker>JOS</speaker><p>	The terms scholarly edition and scholarly editing, what do they mean to you?</p></sp><sp who="#EB"><speaker>EB</speaker><p>	In the broadest sense it’s quite easy or clear to me I think a scholarly edition is a representation of a text or maybe broader a cultural heritage work with some sort of scholarly annotations so these annotations usually serve the purpose of clarifying the work, contextualising it, providing the necessary information to understanding it better. Yeah, so that’s how I would define a scholarly edition and even beside the digital that’s just no matter what form it may take.</p></sp><sp who="#JOS"><speaker>JOS</speaker><p>	What do you think digital editions do well at present, and what might be done better?</p></sp><sp who="#EB"><speaker>EB</speaker><p>	Ah, well I really like this question I think it’s one of the things that we are always thinking about and complaining about what goes wrong or what could be better. So let me first say what they do really, really well and that is just the way they can provide different perspectives on the same text, the way they can present the text in a really clear way much better than most printed editions can, the way they can allow users to select the view that they are most interested in so a reading text or a very detailed text with a lot of information about materiality or whatever, so I think they do that really well and they that’s definitely something that they have over the printed form. The downsides or at least let me put them as challenges is that I think they… it’s a challenge for us to see how we can create digital scholarly editions that are of interest for the bigger public. So often we and when I say we I mean scholarly editors, we create editions for our own group of peers, for students probably as well but I think there is a lot to be gained from thinking about how the texts or the cultural heritage object that we are the thing can be of interest for a larger audience and this is especially because we now have the digital medium at our disposal that offer so much more options so that’s something I think we should be really working towards. A second one I’m sure you are quite familiar with that is the fact that about twenty years ago when digital scholarly editing really took flight, everyone talked about how the data could be reused repurposed and so far, I think I really have to scratch my head thinking about an example of that. So, I think that is something either we should just say ok that’s not going to work or that’s something that we should definitely focus on. And the final thing is long term preservation so that’s a big challenge. I’ve been involved in setting up a project for providing fair guidelines for the fair access to digital scholarly editions because a fair interpretable, retrievable these guidelines they provide a lot of ways in general for scholarly work to be long term preserved, open access etc but not specifically for the type of data that digital scholarly editions consist of. So yeah that’s the third point so long term preservation and accessibility.</p></sp><sp who="#JOS"><speaker>JOS</speaker><p>	Have you any sense of how we can better engage the public with these things, is it something in the way that we build the editions?</p></sp><sp who="#EB"><speaker>EB</speaker><p>	Actually, I think it’s about the people that we work with so I would say reach out to non-scholarly editors, include them in the process of making, include them and their thoughts into the process. When you are busy creating an edition, you easily just focus on transcribing, annotating and encoding, visualising etc but really in early phases of the work you could start talking to artists or writers or performers and ask them, ‘so if you have this text or if you have this work how would you… what would you like to do with it?’. So, I think asking that question to people who are much, much better than scholarly editors are in general in thinking about how creative they can be with the text so that’s a first. I think it’s also about the accessibility after an edition is published, is launched, so how do you make the data of the edition available. I know of at least one good example of the Canterbury Tales project they do this they have this the CantApp and they have asked an actor to read aloud the text in original English which I think is a really great way to take this ancient text, well not ancient but really old text and present it anew and also a great example of how non-scholarly editors can really be of value for a scholarly editing project. So another example I can give is a project that I have been involved in when I did my PhD in Antwerp at the Centre for Manuscript Genetics, we created together with a web development team like an interactive, well, I would say it’s a spinoff of an edition, it wasn’t the edition proper but we presented three different ways for readers to browse through the documents that this specific edition contained and this spin off has a permanent place in an exhibition in the literary museum of Antwerp so it’s for museum visitors it’s there, they can just click on the screen and then swipe through the thinking process of an author and they’re presented with the different documents that we found in the author’s archive. In both cases it involved external people that really helped us as scholarly editors to be better so that’s something that I think we could do quite easily.</p></sp><sp who="#JOS"><speaker>JOS</speaker><p>	Thank you. A lot of your work has dealt with issues of inclusive design, usability, and access; I wonder could you speak a bit more to those topics?</p></sp><sp who="#EB"><speaker>EB</speaker><p>	Yeah of course. So I’ve been lucky to have been an early career researcher in the Marie Curie training network called DiXiT and the goal of that was to have a sort of European consortium in which we learnt everything that has to do with Digital Scholarly Editing and after two years in this group me and a group of other early career researchers, we noticed that the topic of access was really something that wasn’t as present in our training as we figured it should be. I guess it’s because it’s fairly new so we decided to instead of having this prescriptive way of discussing access to really put the question to the community at large. So access is something quite vague it’s hard to define, no one can really argue against it if you say ‘do you think we should focus on access, providing more access’, everyone will nod and say ‘yes of course, accessibility is important’ but we wanted to know what the community actually felt about how did they find access because only then did we feel we could really set up some guidelines so this survey we set out I think it was 2016 so quite long ago already but the data is really interesting and I think the main thing it showed was that access was really broadly defined like almost, from the top of my head, 80% of the respondents defined access as open access so they discussed the idea of putting up paywalls around scholarly work and the way sometimes it was really difficult to gain access to the underlying data of a scholarly edition even though that is something that scholarly editors normally pride themselves on. Another topic that arose from the survey was the web accessibility so again there was no judgement here because I think as a scholarly community we just set out to experiment and do what we could and scholarly editors are no web developers but as a result the idea of, you know, blind people, people with vision impairments, they really were not part of the conversation early on, so how can we make this online edition still accessible for people with different kinds of impairments. And a final thing that I found a really interesting result from the survey was the access to the code behind the edition. So of course it gives rise to a new question, what is the code that we mean, the encoded transcriptions so perhaps an edition is based on a TEI XML so we could say that we provide as editors access to the underlying TEI XML or is the code also the graphical user interface around an edition because of course the interface is also an argument it’s a way of presenting a text and by presenting something you obviously also leave some stuff out, you make choices when you design this interface. So, if you consider the interface as an argument, should you also provide insights into the way you designed the interface? And finally, if you have text analysis tools, if you have used them to analyse your text or even if you have incorporated these tools in an edition, an example of that would be like a digital manuscript programme project who use an automated collation tool called CollateX and they have incorporated the tool in their edition so that users can actually do an on-the-fly comparison between different versions of the text from Beckett so there is a tool that is externally developed, they also provide access to that code. So you see, while we tried to come up with some answers to the topic of accessibility with the survey, it actually gave rise to many more questions and we realised that we needed much more definitions. So actually, last year I set up a new survey with colleagues from the research institute where I work now that was much more focused on the code so if you like I can talk about that as well. So this is a broader survey that questions the definition of code literacy in Digital Humanities but of course it applies as well to code in scholarly editing. We first of all asked the respondents and we got almost 400 completed responses which was pretty cool. We asked them to define code, we asked them to define the importance of knowing how to code for digital humanists whether they thought it was something that digital humanists should be able to do or not and if so to what extent etc so it was also a really interesting survey and to circle back to scholarly editing I think we are still very much in this changing field where I think it’s undeniably important for scholarly editors nowadays to know a little about coding but the question remains how code literate they need to be and what the value of collaboration is.</p></sp><sp who="#JOS"><speaker>JOS</speaker><p>	Have the results of that study been published?</p></sp><sp who="#EB"><speaker>EB</speaker><p>	Yeah, actually we got accepted in the Digital Humanities Benelux Journal but I will send you some links because we already published a little bit about the survey and we gave presentations at conferences about that.</p></sp><sp who="#JOS"><speaker>JOS</speaker><p>	What was the consensus, do people think they should understand the code?</p></sp><sp who="#EB"><speaker>EB</speaker><p>	Well ok so with all of the usual disclaimers the consensus was that overall people, yes, that knowing a little bit how to code was important. Of course, you can say well people taking this survey have already some ideas so it’s kind of a self-selective aspect of the respondents. We have tried to circulate the survey as wide as possible but it was disputed over several lists of course these lists like humanists or digital classists it’s already some… the readers have a mindset but overall, it was important yes. I think one of the results was also that they emphasised the importance of collaboration so the conversation between a scholar and a research software engineer or a web developer was, I think, considered vital in order to have a productive conversation you would have to have some level of knowledge but it doesn’t necessarily mean that you know how to program in Python rather it is having a bit more of a high level knowledge of how programmes work and their limitations as well in order to avoid this very classic distinction between a scholar who is like ‘well I have this idea I think I would like this and this and this’ and a developer or software engineer sighing deeply and thinking ‘oh god this is impossible how am I going to tell you?’</p></sp><sp who="#JOS"><speaker>JOS</speaker><p>	You mentioned automated collation; on the subject of computer assisted techniques and machine learning, in the broadest sense, do you think these will play a greater role in digital scholarly editing in the future?</p></sp><sp who="#EB"><speaker>EB</speaker><p>	Well automated collation of course it’s a method to examine text so it depends on the type of texts you work with whether it’s useful, if you have one version there is very little to collate but let’s say you have a work and the work exists in different versions and these versions are more or less similar then of course yes having a software can help you compare these texts, that’s incredibly important. I’ve been collaborating a lot with Ronald Haentjens Dekker who is currently the lead developer of CollateX, a text comparison software and I really like the way Ronald, as a software engineer, collaborates with scholars because his perspective on the matter is that scholars should actually know how to code so CollateX has been designed as something that you can tweak as a scholar depending on the needs of your text and your research objective. But the tweaking means that you need to program it a little bit so there is different parameters you can change and, well, knowing how to do that is really important. So what we’ve been doing over the years is giving workshops on CollateX it is an open source software so everyone who uses it and who would like to give a workshop on that can, so it’s not owned by Ronald or by our institute and this, I don’t know, it’s set up really simple so it lends itself well for, you know, teaching non-code literate scholars to code a bit. There is a Python version of CollateX and Python is yeah, it’s quite an intuitive programming language for non-programmers. It is set up as a sort of pipeline with 3 or 4 or 5 different steps so in each step you do something to the text, to the input text, so first it’s tokenised and it’s normalised if you need to and it’s compared and then you have a moment where as a scholar you look at the first results and you’re not completely satisfied so you go back and you tweak the comparison algorithm and there’s different ways to visualise the result, so these steps, they make it really easy to get involved as a non-programming scholar and they also, I don’t know, I found that it’s really helpful to teach people how a programme works, how a software programme works and how it this case how a text analysis tool works and how there’s different moments (I’m sorry I’m looking for the right words, sometimes my English completely disappears). So how it processes your text and the different ways in which your text is processed and transformed. </p></sp><sp who="#JOS"><speaker>JOS</speaker><p>	Again, this is quite speculative, but could you envisage a future where other aspects of digital editions are automated, things like semantic annotation? </p></sp><sp who="#EB"><speaker>EB</speaker><p>	I think that’s the path we are on currently so I can definitely envisage a future I just think it’s very important that we as a scholarly community are not too keen on simply adopting software that automates or processes because if we look really carefully at a software, at a text analysis tool and try to critically analyse the different transformations that our text undergoes when it’s being processed by such a tool, we can learn so much. And I think that this part of learning to work with tools and learning to work with software is invaluable for us to become better scholars but it’s also often overlooked so there is many great text analysis tools out there, there’s loads of tutorials being given on such tools but not many of these tutorials actually go into the transformation process and I think that’s a shame and I maybe that’s something we should focus more on while we are on the path towards automation let’s make sure that we are always aware of the different process that are applied to our text and what we lose and it’s a decision making process so every step in this way either a scholar or the tool makes some decisions for you and if you are not aware of these decisions being taken then it’s really hard to actually validate the results. And of course this is even not even talking about machine learning tools because there it’s sometimes you don’t know the decisions being made that’s the whole point so I’m not sure how to be mindful and critical about that process but I think in general it’s really important we do that.</p></sp><sp who="#JOS"><speaker>JOS</speaker><p>	Thank you. Can you tell me about TAGML and this idea of hypergraph modelling?</p></sp><sp who="#EB"><speaker>EB</speaker><p>	Yeah, actually yeah really, it’s a good segue to what I just said because TAGML or ‘Tagamel’ as we say it is really started out as a thought exercise so it’s becoming aware of the influence that our models have on our thinking. I think for 90%, just a rough estimation here, most digital scholarly editors use XML to model their texts and of course I do because almost all tools are XML based, we have the TEI which is currently based on XML so it’s normal and expected that XML plays this really big role in our text modelling but at the same time people have warned for the influence a model has on the way we think about texts so XML is hierarchical so we really easily start to look at our cultural heritage object to be modelled, we start to see the hierarchy in that but in some cases it’s really difficult to use this hierarchical model, not all texts are hierarchical on the contrary and TEI XML the guidelines they offer numerous ways to circumvent the hierarchy but if you would take two or three steps back and leave the three model aside for a moment and start thinking just conceptually what would be the best model for this text, well, in many cases it turns out that this is the graph model, a non-hierarchical relational model and this is how TAGML started. TAGML is a hypergraph which means that it’s like a regular graph but it’s a bit more powerful and it’s a really great way to add annotations to parts of the text that are not linear. So let’s say we have a whole sentence and you would like to mark up several words in that sentence that are not consecutive, you can do that in XML by using attributes you know pointing from one element to the next element to which it is related but in a hypergraph you don’t have to do that, you don’t have to use these attributes which means that the way your data is modelled and stored in the computer is actually more general so it’s easier for a variety of tools to process your data because the tools do not have to know about your specific choice in attributes and attribute values. It’s hard to talk about this in a generic way without giving examples but overlap is the example we usually give so if you have the hierarchy has been broken because you have a paragraph that runs over two pages for instance and, I don’t know, you’ve chosen to focus on the paragraph, you would have to use a previous next attribute in TEI XML and add that attribute to the paragraph element but if you then later on have a text analysis tool, you would have to let that tool know to inform that tool as it were that you have used the next attributes in order for these paragraphs to be considered as one and that means that these generic tools will need to know about your specific encoding. Now in hypergraph you can use overlap in markup that’s not a problem because it’s not a hierarchically ordered graph so you don’t have to bother with using attributes to state that the two paragraphs are actually one paragraph and that makes it a lot easier for your encoded text to be processed by different tools.</p></sp><sp who="#JOS"><speaker>JOS</speaker><p>	Do you have an example of an edition that was done using this approach?</p></sp><sp who="#EB"><speaker>EB</speaker><p>	No well I’m actually working on one but it’s not published yet because TAGML is still under development and we are currently in the validation phase so that’s… we have developed a way to validate TAGML encoded texts but there is still some work to be done before the validator is complete that means that, well, it cannot be launched yet. So this is why I would really like to see TAGML and its data model hypergraph as a thought exercise, using it really reminds me of you know to think about the models we use to model our text conceptually and in addition to TAGML I am still using XML and you know I’m a happy XML user and it’s not a competitor we would never want to present it as a competitor but just ,you know, something that exists beside XML and can be used by people who find that hierarchical model is not suitable for them. We’ve been inspired by TexMex and by XML so it resembles a bit TexMex and a bit of XML. I’ll share some links to the system. What I really enjoyed doing was 2019 because it was before the pandemic was we gave a TAGML workshop conference and it was really great to do because we could do exactly this, we could tell scholarly editors ‘ok forget everything you know about TEI XML, take a fresh look at your text, think about the different features you would like to model and here we have this really powerful model for you that actually transcends XML in a very intuitive way’, it was a great exercise and also good to see how people who are not closely involved in the development of TAGML responded to it because I knew it all from the beginning so I’m obviously very biased. So realistically even if TAGML fails I would still consider it a success just because it helps you reconsider tools you use and that’s the most important thing. </p></sp><sp who="#JOS"><speaker>JOS</speaker><p>	Do you think we might ever see TEI as a standard move away from XML, maybe adapted to a less hierarchical system? Say a Markdown implementation of TEI, for example?</p></sp><sp who="#EB"><speaker>EB</speaker><p>	Yes, theoretically I think, yes, it’s possible. I mean famously TEI always says we’re not married to XML, we were SGML before so we could change. It’s going to be really difficult I think especially because as I said there is so many tools, there’s this whole infrastructure, there’s this whole environment that is now XML based so any language whether it’s markdown or TAGML or something else that we cannot conceive of will need to be a proper alternative but you could say like we have done with TAGML is built an XML converter so you can convert your TAGML texts into a simplified XML and because that means that you have this really rich type of encoding and you transform it into a more simpler XML you would have to, again, you would have to make choices, what elements do I consider important, what information can I leave out of my XML export and also which hierarchy will I chose to be leading. What we’ve done now is to give the user the choice to select a leading hierarchy and the rest of the elements that that do not fall into that hierarchy are modelled as milestones and it works great. So in a way you can say I can envision a future in which people will model their texts in TAGML and the moment they start using tools they would have obviously a proper idea of what they want to use the tool for so they transform part of the TAGML text to XML just as much as information is required for the tool to do its analysis and then the text remains, as it were, in a master TAGML file.</p></sp><sp who="#JOS"><speaker>JOS</speaker><p>	And is the element vocabulary something you devised yourself or is that based on TEI?</p></sp><sp who="#EB"><speaker>EB</speaker><p>	Out of ease we opted for TEI but we figured you can create your own schema like you can with TEI and that makes sense.</p></sp><sp who="#JOS"><speaker>JOS</speaker><p>	So is TAGML the future?</p></sp><sp who="#EB"><speaker>EB</speaker><p>	It could be yeah. Our biggest challenge now is human power, I wanted to say manpower but that’s not it like I mean we need more developers. I’m not a developer, I’m a digital scholarly editor, I mean I can code but not that good.</p></sp><sp who="#JOS"><speaker>JOS</speaker><p>	Who’s involved in this project? </p></sp><sp who="#EB"><speaker>EB</speaker><p>	So it’s Ronald Haentjens Dekker the same person who developed CollateX and we have a great, very smart developer but he changed departments unfortunately so now it’s just Ronald and me and we need more developers to actually help us. So that’s why TAGML at the moment it’s in a sleeping state.</p></sp><sp who="#JOS"><speaker>JOS</speaker><p>	You’ve been very generous with your time, so I’ll finish up now with one final question. Magic wand scenario, what should future digital editions look like?</p></sp><sp who="#EB"><speaker>EB</speaker><p>	Ok that’s a great question, I really like that. So the digital scholarly edition of the future I think allows for an easy repurposing of the data, it allows for different spin offs, it allows for artists and scholars alike to use the data as they see fit, we will in this magical future have devised ways to preserve the data long-term, we will have devised ways to create access points into the data so it can be reused by others. But most of all I really hope it will gain a more permanent place in the literary world and that’s me just talking about editions based on text, again if you look more broader at cultural heritage objects, I would say maybe these scholarly editions will have gained a place in the cultural heritage world. I think I would really love for us to collaborate more with people outside our circle, outside our bubble in order to really shine new lights on these historical works, you know, the repurposing would be more than one scholar repurposing the texts created by another scholar, it would really be more of a creative endeavour.</p></sp></body></text></TEI>