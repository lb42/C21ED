<?xml version="1.0" encoding="UTF-8"?><TEI xmlns="http://www.tei-c.org/ns/1.0"><teiHeader><fileDesc><titleStmt><title>C21 Editions:  Christopher Ohge</title><title type="sub">Interview conducted by  James O’Sullivan		 on January 12th, 2022</title></titleStmt><publicationStmt><p>Unpublished working draft</p></publicationStmt><sourceDesc><bibl> O'Sullivan, J., M. Pidd, M. Kurzmeier, O. Murphy, and B. Wessels.
                            (2023). <title>Interviews about the future of scholarly digital editions
                                [Data files].</title> Available from <ref target="https://www.dhi.ac.uk/data/c21editions">https://www.dhi.ac.uk/data/c21editions</ref> (File C21_JOS_CO.xml downloaded: 8th June 2023).</bibl></sourceDesc></fileDesc><encodingDesc><p>Retagged in TEI P5 from RTF (via soffice and docxtotei) </p></encodingDesc><revisionDesc><listChange><change><date>$today</date>Header added</change></listChange></revisionDesc></teiHeader><text><front><div><head>Interviewee bio</head><p>CHRISTOPHER OHGE is Senior Lecturer in Digital Approaches to Literature at the University of London’s School of Advanced Study and Co-Director of the Herman Melville Electronic Library.     </p></div></front><body><sp who="#JOS"><speaker>JOS</speaker><p>	What do the terms scholarly editing and scholarly edition mean to you?</p></sp><sp who="#CO"><speaker>CO</speaker><p>	Thanks again for including me. As I said, I think this is a really important endeavour. To answer that question, the problem is, I think that what a scholarly edition now is, is very much unclear in a lot of ways. For a long time a scholarly edition was a preparation of an accurate text with apparatus of various kinds and the apparatus was usually apparatus criticus with textual variants, and sometimes (but not always) contextual notes and various other apparatus tools like historical notes, contextual notes, textual notes. Of course for centuries that form of editing was tied to book technology, to codex technology, and … because of digital technology we’ve now gone outside of the confines of book technology and we’re operating in this very fuzzy space where the shape and the form of the edition is now not tied to the physical book, so a lot of those traditional apparatus features might be irrelevant in a digital scholarly edition. Perhaps the new form of the edition will not even be used for, say, reading purposes, but rather for different kinds of investigating. Hans Walter Gabler has talked about this, that the new scholarly edition would be a digital tool that complements book reading, but I still think we’re grappling with what that means. But in certain respects I do think that there are certain core concepts in the scholarly edition that continue even in the digital space. Barbara Bordalejo has talked about this really nicely, that there are certain practices that haven’t changed, namely even a digital editor is really concerned with accuracy, and is concerned with philological concepts of tracking the details of text development by looking at different witnesses. But I do think the form of the edition is really problematic at the moment, and that’s why the publishing question gets problematic, because if you can’t fit the edition any more into the book, then what are you publishing? And how will you do it?</p></sp><sp who="#JOS"><speaker>JOS</speaker><p>	What do digital editions do well, what might be done better?</p></sp><sp who="#CO"><speaker>CO</speaker><p>	What they do well is, I think, on a very basic level, searching through texts very quickly. So, if you are able to search for a word in all existing copies of Walt Whitman’s ‘Leaves of Grass’ using the Walt Whitman Archive, that is an extremely important advantage, so querying information is the greatest advantage. It’s a basic function but it’s still incredibly useful. Most scholars I know of who don’t create digital editions per se but use digital resources say they want to be able to query lots of information. Now, the ease with which that can be done is another question. Another thing that I think is very advantageous in digital editing is that you can modify clunky aspects of print textual editing so that they’re more readable and useful in a digital format. On the Melville Electronic Library project I think one of the best things we’ve done was to convert a print genetic edition of Herman Melville’s Billy Budd--which was transcribed using these really odd symbols for editions and deletions and transpositions and things like that--into TEI XML code, which is then rendered into a diplomatic transcription. We used a software called TextLab that shows the genetic attributes that were hitherto in these weird codes in the print book and they now appear as pop-up notes over additions and deletions and things like that. Clearly the digital format is so much more useful to a reader, and they don’t have to worry about these cumbersome symbols that some mid-20th Century print editor conceived to represent genetic information. So, using semantic encoding to represent certain aspects of the apparatus that used to be rendered in print are hugely advantageous as well, in addition to the fact that once you have that semantic encoding you can process it, you can query it using XPath and come up with all kinds of interesting insights. Another advantage is just being able to juxtapose a facsimile of a manuscript with a transcription next to it. You don’t have as many space constraints, you have the advantages of image software like OpenSeadragon where you can zoom in and out of images, and now with IIIF you can do even more interesting stuff with images. Mirador, for example includes metadata with those images.</p></sp><sp who="#JOS"><speaker>JOS</speaker><p>	In terms of what which could be done better; given a magic wand scenario what would you like the future digital edition to look like?</p></sp><sp who="#CO"><speaker>CO</speaker><p>	Now that's a difficult topic. I think the biggest problem that we have in digital editing is that so much energy has been devoted to trying to replicate book reading into screen reading and I just don't think that works. There's plenty of cognitive science research right now that shows that we're absolutely rubbish at reading comprehension on a screen, so why would you spent a bunch of time trying to mimic the document or the book on a screen? And I think that most people, myself included, don't consult a digital edition because they want to spend an hour or two reading something in a linear fashion. I would think the vast majority of the time you're trying to pinpoint information through querying various kind of things, whether it's faceted searching based on the semantic encoding or just simple word searching or by using other interface tools, whether it's network visualisations or whatever kind of visualisation you can come up with. So, I do worry that we still have a bit of a problem of sedimentation where we're kind of trying to bring the metaphors of reading in book form into the screen. I've long thought that one of the best digital projects that I know of, that I've used almost on a daily basis, is the Oxford English Dictionary Online, and it's not because I want to read the dictionary, right (laughs). It's really good at giving you precise information with just the right amount of links to other important bits of information. It's not really there for reading; it's there for laser-focused attention on something particular, but it's also designed in a hierarchical fashion which I think is better for our brains when we're dealing with screen information. So, I'd like to see, and I don't know what this means yet because it's sort of blue sky thinking, but I wish more digital editions were like the OED online (laughs).</p></sp><sp who="#JOS"><speaker>JOS</speaker><p>	Wonderful analogy. You mentioned a tension between scholarly editing and publishing; would you be willing to expand on that?</p></sp><sp who="#CO"><speaker>CO</speaker><p>	Yeah absolutely, thanks for bringing that up. I just want to say I was amused by your point that you still want the physical book because I'm exactly like that too. I mean there's something weird about people like you and me who are really big on digital stuff always order the print copy on top of that. I would not want to read my own book in pdf on the screen. I have spent quite a bit of time thinking about this. Basically my theory of the situation is that -- and this was backed up by the way by people I talked to at Oxford University Press -- the TEI (Text Encoding Initiative) guidelines were conceived by the Poughkeepsie Conference in 1989, I think. A lot of the people that were driving that initiative were at Oxford and they were also consulted by Oxford University Press when they started their own digital initiatives, and there was a chance at that point to bring together publishing and TEI. I bring up TEI of course because that's the standard that most of us are told to use when we're creating digital editions. I use TEI, I teach TEI, but the fact of the matter is that TEI is not an acceptable standard in the publishing industry and the reason why is that digital publishing still operates under a print-first or book-first mentality of production pipelines, and what that means is they need predictable content models. In order to do what any business would need to do to make money, namely increase volume and reduce cost, they need to streamline the data model for predictable content, and most people in publishing now who are making money are putting out scientific article models, which are basically prose content models with certain features like keywords and references and all of that, but since about the early 2000's, I believe, those systems have been developed in digital publishing to handle that kind of content and to make sure that all of this information is interconnected. They use XML, but it's a really simplified XML and again it's predictable and scalable so it can handle all kinds of subject matter as long as it stays within that content model. TEI XML on the other hand is completely agnostic about form and its tag set is massive -- the last time I checked it's 590 elements, thousands of possible attributes within those elements and vocabularies are never controlled except within projects typically. What you have is a very flexible content or data model that could produce all kinds of forms and formats whether it's dictionaries or surrogates of books or manuscripts. And what happened at Oxford University Press was that they considered using TEI early on but they saw that it wouldn't work because it was too flexible, and they couldn't scale up a system like that. So if you look at what digital publishing tends to use now, which is BITS XML for books and JATS XML for journal articles, look at those data models -- they're really simple, they're simplified just like HTML is simplified and that's how they can increase a lot of content. If you had TEI XML in publishing, you would essentially have to build a new publishing system from scratch each and every time and I know of no publishers that have the resources to do that. So what's happened is that since about the early 2000's you've had completely divergent tracks or I'd call them parallel tracks (because they're both doing interesting and innovative stuff), but on one track you've got editors and scholars who were into DH, early DH or humanities computing even in the 90's who were creating TEI-SGML and XML editions outside of the confines of the publishing industry, and then on the other track you had people in the publishing industry adopting various information technologies to make their business more efficient, and what that meant again was simplification, scalability, and predictable content models with existing software. Essentially the TEI scholars were creating these extremely bespoke products that needed to be built from the ground up and then digital publishing which issued such bespoke enterprises. You've got a system now where the only successful digital scholarly editions are ones that get huge amounts of grant funding because that's the only way you can successfully build these publishing systems from the ground up because you need scholars who know how to edit, who know XML, who know a little XSLT and then you need web developers and you need cooperation from IT, University IT, whose attention is often split in various places and you need the maintenance costs, which are not insignificant, so I don't think that's a good situation. It's great if some projects get grants and can do really amazing things but I don't think that should be the prevailing paradigm. The whole point of having publishers like OUP is so your average scholar can submit their scholarship and get it done and presumably in the worst-case scenario you can just do it using your research time and they will publish it for you and out it goes into the world and you get support. I know that support in the publishing industry has been declining but they still provide significant support. Even a terrible publisher that jacks up their prices unfairly and doesn't really help you with copy editing or fact checking, even their marketing is useful and particularly any edition that's published with a print publisher gets catalogued in University Libraries and deposited in the British Library. Find a digital edition that's in a university catalogue – very few of them are. They're very hard to find; they're still very difficult to discover and that's because they're largely functioning as self-publishing projects.</p></sp><sp who="#JOS"><speaker>JOS</speaker><p>	And how do you think we go about resolving this? Is it as simple as a more simplified TEI or is a wholesale cultural shift within disciplines required?</p></sp><sp who="#CO"><speaker>CO</speaker><p>	I felt thought the problem I was drawing attention to in the public sphere was really difficult to solve because, on the one hand, what I value about TEI is its flexibility: you can encode things in the way that you see fit in whatever vocabulary works for that project; of course TEI can be modified and any TEI project is open source and tool independent. Those are all great things, but that data is not useful to the vast majority of people unless it is mediated through an interface and publishing gets us that interface. And if publishing support isn’t there, we have a serious reckoning. What I would like to see… one of my proposals is that institutions with existing infrastructure that can handle this sort of thing, namely university libraries, adopt something like a TEI publisher type software and find a way to partner up with some sort of publisher to sign off on the quality of that kind of work. There are examples of this working out already, UVA Rotunda press does it. When I was at the Mark Twain Papers, we had this situation very nicely with the University of California Press and California Digital Library and the Bancroft library all working together to produce print editions and digital editions that were peer reviewed and published by UC Press; that’s a nice situation. That does require a lot of cooperation and, again, resources, which really is an issue. So ideally something like that could work out because then the library becomes a sort of publisher -- this has happened before, so it’s not like it’s impossible, right. I think that’s one good solution. Another solution that I propose is that if you’re working on a digital edition and you don’t have that infrastructure and those resources then you don’t think about TEI as a first step and instead we start thinking about minimum viable product, which would mean using a minimal computing approach. And what I like about this approach is twofold: one, it's much easier to publish something using minimal computing solutions -- namely a Markdown-based data model using static websites -- but you can also use TEI in minimal computing approaches as well if you know enough XML. It’s not that hard to use a software like CeTEIcean to process your TEI XML in the browser and create a static website from there. So, what I’m saying is not anti-TEI in other words; it’s just thinking of minimal computing solutions instead of server-side applications, which are the norm at the moment. And the second thing about minimal computing has to do with the resources and environmental impact of digital projects -- this is something I feel receives very little attention. We’re starting to learn that the amount of resources that digital projects take up is potentially immense and when you run server-side applications, you know, each time you’re calling to that server and it’s reloading pages and it’s reloading the website it’s taking enormous amounts of energy. As Alex Gil has pointed out, people in the Global South might not even be able to access these resources because of low bandwidths, and a lot of them are using mobile phones to access digital resources, so minimal computing could serve a dual purpose of being environmentally conscious but also getting us away from this notion that the richly semantically encoded edition is the gold standard for scholarly editing (because I don’t think that needs to be true in every case). And again, I was worried this might happen when I wrote my book that someone would interpret it as being anti-TEI; it absolutely is not. It’s just pointing out that the TEI is resource intensive and expensive and why the publishing industry hasn’t adopted it. So, I think that TEI is a great option if you have like the resources of the Women Writers Project or the Mark Twain project or the Walt Whitman Archive or, you know, various projects in Germany have excellent TEI publishing systems that were built but they’ve got the resources -- fine, that’s great, but I don’t think that’s the solution for the average scholar.</p></sp><sp who="#JOS"><speaker>JOS</speaker><p>	Why not just translate one of the Lite versions of TEI into Markdown? I mean, TEI isn’t XML dependent, but XML can present some challenges for smaller publishers and individuals. I’m not saying that’s the solution, but why not move our principles to more intuitive standards?</p></sp><sp who="#CO"><speaker>CO</speaker><p>	You're absolutely right, and I've pointed this out when I taught a few years ago. I started changing my course of digital editing to start with markdown, then HTML, then we'd finish with TEI to mimic the spectrum idea that I just mentioned, but yes, I've shown my students how you can create semantic markup in markdown; it's not that hard. I've also shown my students how you can use a content management system like Scalar to mimic semantic tags in that system and export all of that data in RDF-XML. You don't need TEI to mark people and places; you can do it using Omeka or Scalar or whatever and it does the same exact thing. The data is a little different but who cares when you're focused on usefulness. But, I agree, I like TEI Lite, anything that looks more like HTML5 is useful because it's again easier to publish and there are various ways around creating semantic tagging of various ways in your scholarship but what the TEI people will tell you, and I kind of agree with them, is if you put as much of the scholarship in the data, you're preserving all that scholarship in one place. And in theory I agree with that, but the problem with that though is the more data you put into that one file the harder it's going to be to publish it. So you need to square that circle at some point and this goes to an issue that my colleagues at the University of Chicago, David and Sandy Schloen, who run the what's called the OCHRE database, had pointed out in their criticism of TEI. They're very much about creating database projects instead of document-oriented encoding projects; they say that, and I think they're right, that TEI was sort of designed by librarians and archival-minded folks who thought this was a primary means of keeping as much information about the document as possible in one place, but because it's a hierarchal encoding system you can't actually get all of that in there without breaking some hierarchy at some point, and with a database approach you can just atomize and identify every feature from the word down to the letter and going all the way up to the entire works in a non-hierarchical database -- the sky is the limit as far as what you can include -- but you're not worried about hierarchies or anything like that and it's easier to publish that from that kind of system. On the other hand, TEI is not a generalised solution for dealing with all textual problems, but it also creates a lot of problems for publishing the data, and that's when it gets really complicated.</p></sp><sp who="#JOS"><speaker>JOS</speaker><p>	I wonder if you'd be kind enough to sketch out some of the other key ideas in your book, and perhaps, in particular, what you call pragmatism and the role that it should play in rethinking data modelling and editorial principles? I believe you also tie this into broadening the conceptions of the edition?</p></sp><sp who="#CO"><speaker>CO</speaker><p>	I suppose this brings us back to the first question of what a scholarly edition is. I have written before that the original word upon which scholarly editing is based is ‘editio’. If you look that word up it doesn’t mean a book; it means an exhibition and an exhibition of various kinds. With ‘editio’ you’re kind of shifting away from confining all this editorial scholarship within the codex and moving outwards into a kind of museum-like setting, like an exhibition of various kinds where you are moving through different rooms that are curated in certain ways, but there’s also a lot of freedom to skip things, go to different rooms, have different views and so on. So, a kind of exhibition of archival experience is what I’m looking for, and I’m thinking about what that would look like in practice. Now, as far as the practical outputs, it seems to me that one of the primary goals of editing then would be -- what experiences can be generated from the texts that I’m working with? That’s where pragmatism is useful because you have to consider what the documentary evidence is suggesting to you, what kinds of questions it is asking of you, what the scholarly consensus on it happens to be, what the potential experiences might be, and so on. That could result in a whole range of exhibitions, one of which could be the very standard reading text in pdfs that’s printed out as a book. Another could be a visualisation like a word cloud or a network graph, the kinds of tools that go beyond linear reading practices. I also want to think more horizontally and relationally so that we’re focusing less on a depth model of scholarly editing where we’re going as deep as we can into that text to find the real proper textual description that matters, of the ur text (if you’re a philologist) or the ‘ideal copy’ text (if you’re a modern critical editor). There’s nothing wrong with that sort of thinking in certain contexts, but one of the things I think they get wrong is that once you start to think about an edition as an exhibition you’re not talking exclusively about the depth model anymore; you’re talking about how it relates to other contexts. This is more of a horizontal model of textual editing, and this is where in practice I like the graph database because it can actually do pan-relationalism in practice. I’d mentioned my colleagues David and Sandy Schloen; this is their model of what textual editing would look like. It’s still in development with my Melville project. But I should also take a couple of steps back and explain clearly how I’m employing pragmatism because it’s an ordinary word that people use to describe doing a sensible or common-sense thing, but that’s not really what I’m getting at. It’s a certain method of thinking based on capital P Philosophical Pragmatism. For me it’s twofold: one, it’s about methodology, how you think about preparing editions and gathering all of the evidence and testing that evidence against the prevailing consensus of what people have said in the past about it, and thinking of new ways of tying it to ‘experience’, which is how can you get yourself closer to the composition of the text, the history of the text, and what editors have said about that text. So, you’re kind of entering into a conversation about that text rather than receiving the definitive edition of it. I know I’m not the first person to say that we’re done with the definitive editions now, but what I haven’t seen is how digital editing can facilitate your entry into that conversation in really interesting and productive ways that weren’t possible in books. And the other part of pragmatism is the pragmatism of modelling your data so that it can be useful for different kinds of publication techniques, and that’s where the range of options comes in: what interfaces and tools are most useful for the documents under consideration. Ideally for me this means creating a set of complementary tools from reading tools to investigative tools to critical tools and this is where the exhibition metaphor is useful. An exhibition can mean a standard reading text, annotated transcriptions, diplomatic transcriptions of manuscripts, book-historical or bibliographic descriptions of printed works, visualisation tools like network graphs and word clouds, keyword-in-context searches, text analysis tools, database tools for querying, maps, audio visual material. But pragmatism also asks you to consider what is practicable given the resources that you have. The practicability of creating these tools is often hindered because of, again, the lack of publishing support so many of us have in this space. Does that make sense?</p></sp><sp who="#JOS"><speaker>JOS</speaker><p>	One of the things we’re trying to do with the C21 Editions project is to really look at the fact that digital editions don’t typically avail of and computer assisted analytical techniques.</p></sp><sp who="#CO"><speaker>CO</speaker><p>	Yeah.</p></sp><sp who="#JOS"><speaker>JOS</speaker><p>	I wonder if you could speak further to that, the use of Natural Language Processing and Machine Learning as being embedded in editions? Is that too impractical from a publishing perspective?</p></sp><sp who="#CO"><speaker>CO</speaker><p>	I think it’s an interesting avenue to consider; I honestly don’t know enough about machine learning. I’m interested and intrigued, and I’ll probably experiment with it myself at some point, and just to be clear, one of my provisos of pragmatism is to experiment all the time because as soon as you experiment you might change the consensus (and that’s actually a good thing). But I do worry about machine learning from an environmental perspective; it’s hugely resource intensive and I have to wonder how important it is in some contexts to do machine learning just for a kind of ‘oh isn’t this interesting’ sort of result when you’re burning up fossil fuels to generate these things. So, I think we need to be conscious of that but I still think it’s worth experimenting with, just cautiously. I will say that the third chapter of my book talks about re-thinking editing as a complementary activity of text encoding and text analysis and I feel like that’s a bit more practical than machine learning because what I’m suggesting there is that editing itself is kind of computational in that it’s always looking for patterns and outliers and doing quantitative analysis. So the question is, why is it that scholarly editors were kind of obsessed for so long with just representing documents with TEI XML when they could have been doing other kinds of text analysis to generate new insights? So, I think that one of the intermediate steps that could be taken right now is to think of text encoding and text analysis as intertwined because it not only accomplishes a really important thing, namely, you generate new things about your text by doing these text analysis whether it’s chucking a txt file in Voyant or processing some XML using R, Python or doing other kinds of NLTK on texts and generating questions and critical insights based on that. Text analysis also kind of relieves the pressure on the editor to create things like really complicated data models to represent their text when in fact for some types of projects you might get a lot more traction just by taking a really well edited accurate txt file and chucking it in Voyant. I’m sure you’ve had that experience with students where you realise, ‘this is amazing, you’ll love it, it’s fascinating’. I wish there were a little more of that and I think that could be done without having any resource issues, and most people can learn text analysis whether it’s Voyant or AntConc or just giving people some initial instruction in R or Python and having them run through a Jupyter notebook. That can be done fairly quickly and with minimal resources, and I think it would help us get past the kind of narrow-minded focus on TEI XML representation. Ideally, they go hand in hand but again there are much more minimalist ways to pull it off.</p></sp><sp who="#JOS"><speaker>JOS</speaker><p>	I mean, is there any value in semantic tagging? Why does it even exist, or rather, persist, at all?</p></sp><sp who="#CO"><speaker>CO</speaker><p>	I’ve asked myself the same thing (laughs). I’m glad you’re asking this; these are good questions and you’re probably going to upset a few people by asking them, but it needs to be asked. I had an example in my book that may have seemed glib and kind of flippant, but I just showed an example of a manuscript transcription of a poem and I showed the TEI-XML encoding: each stanza is contained in an &lt;lg&gt; element, and it contains &lt;l&gt; elements, and that’s it, that’s the TEI XML poem. I asked rhetorically, ‘what’s the point of doing that if ultimately it’s going to get transformed into &lt;p&gt;s in HTML’? That’s what readers experience, so what is the point? And the fact of the matter is it’s all contingent, these distinctions are ultimately invidious and yes with a certain amount of technical know-how there’s absolutely no need at all to do semantic encoding, you could use Python NLP and automatically encode named entities, and, as I said, you can use a content management system like Scalar or Omeka to tag things semantically. The only problem is, as I said, it is useful in a certain philosophical sense to have a single file that contains all of your interpretations and I suspect that’s what most TEI people would tell you. I think there’s some merit to that point, but as a pragmatist I think there’s also a danger in being so idealistic and abstract when in fact most people are not encountering that stuff; most people are encountering something they use on their computer that has been modified and transformed in various ways with more layers of abstraction after the TEI XML has been introduced. I mean, there’s nothing pure about these data models, and you know all models are wrong, some are useful, but the usefulness question is what’s more interesting -- and difficult.</p></sp></body></text></TEI>