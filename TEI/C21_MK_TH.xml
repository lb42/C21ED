<?xml version="1.0" encoding="UTF-8"?><TEI xmlns="http://www.tei-c.org/ns/1.0"><teiHeader><fileDesc><titleStmt><title>C21 Editions:  Tim Hitchcock</title><title type="sub">Interview conducted by  Michael Kurzmeier		 on March 15th, 2022</title></titleStmt><publicationStmt><p>Unpublished working draft</p></publicationStmt><sourceDesc><bibl> O'Sullivan, J., M. Pidd, M. Kurzmeier, O. Murphy, and B. Wessels.
                            (2023). <title>Interviews about the future of scholarly digital editions
                                [Data files].</title> Available from <ref target="https://www.dhi.ac.uk/data/c21editions">https://www.dhi.ac.uk/data/c21editions</ref> (File C21_MK_TH.xml downloaded: 8th June 2023).</bibl></sourceDesc></fileDesc><encodingDesc><p>Retagged in TEI P5 from RTF (via soffice and docxtotei) </p></encodingDesc><revisionDesc><listChange><change><date>$today</date>Header added</change></listChange></revisionDesc></teiHeader><text><front><div><head>Interviewee bio</head><p>TIM HITCHCOCK is Professor Emeritus of Digital History at the University of Sussex. With Professor Robert Shoemaker and others he has created a series of websites helping to give direct public access to 37 billion words of primary sources evidencing the history of Britain. Designed to underpin the writing of a new 'history from below', these sites include: The Old Bailey Online, 1674 to 1913; London Lives, 1690-1800; Locating London's Past; Connected Histories; and The Digital Panopticon: The Global Impact of London Punishments, 1780-1925.</p></div></front><body><sp who="#MK"><speaker>MK</speaker><p>	Just generally, could you maybe say a few words about what the terms digital editing and digital publishing mean to you?</p></sp><sp who="#TH"><speaker>TH</speaker><p>	On digital editing, my immediate thoughts go back to XML and the kinds of mark-up it enabled, and how it exemplifies the changing ways text is represented on screen. Flat text has come to incorporate multiple layers “marking up” the significance of words and phrases. In some respects, this is just the end of a much longer process.  From the beginning of printing, we have slowly created ways of adding meaning and structure along the way – whether through formatting or linguistic clues.  XML, TEI and all its progeny has just moved this process onwards. </p></sp><sp who="#MK"><speaker>MK</speaker><p>	And digital publishing?</p></sp><sp who="#TH"><speaker>TH</speaker><p>	The “digital” in digital publishing is pretty much meaningless these days. All publishing is digital, and while the formats of published books and articles persist in the visual representation of the texts we produce, none of the machines and processes that determined those formats are relevant any longer. Arguably the whole production and business model that underpinned traditional ‘publishing’ has just dissolved into mush. What was a system of gatekeeping made necessary by the costs of production, has become a new kind of democratic broadcast medium.  I will be showing my age, but my first book, published in 1987, was set on a Linotype machine, and went through galleys and two sets of proofs, all hard copy, all posted, and copyedited with a pen in hand, using an arcane set of proofing symbols.   This system of academic publishing – and the stringent gatekeeping that went with it - simply no longer exists. We still rely on the same old formats, but they are a kind of dead man walking – the ghost of an older system and technology kept upright by the Academy itself. </p><p>We have had endless attempts at innovation – one can point to the rise (and fall) of the academic blog in the 2000s and 2010s. But for the most part, we have continued publishing doorstop monographs and ridiculous collections of essays, costing £110 a pop, made commercially viable by library budgets and print-on-demand. I suspect that this conservatism – the failure of innovation in the formats of academic publishing – result from the systems of authority in the Universities that ensure every ECR needs a single authored book, and every promotion, tenure case, and REF cycle needs another.  And that change in the format of academic publishing will probably only come when the Academy as a whole figures out new ways of assessing and sharing research; and as significantly, when readers in general have developed new  ways of judging the authority of a text.  This is not a conclusion I come to happily.  I have had my fingers burned so often in trying to innovate in the forms of digital publishing that I have become jaundiced.  In the 1990’s me and a lot of others, thought we could reinvent everything. Narrative was going to change, and we were going to have endless alternative storylines.  Then in the 2000s it was hyperlinks and linked open data and its promise of the ability to tie sources and evidence directly to text. And in the later 2000s and 2010s, blogs and self-publishing seemed to promise a world where everyone could be their own University press. All of these possibilities, these mini-revolutions, feel as if they have come to nothing, and that the world of academic publishing has become more and more conservative. This is partly driven by the underlying business models, but it is also driven by the structures of the Academy and the demands of the Academy for conservative forms of publication in order to preserve conservative forms of authority and hierarchy. </p></sp><sp who="#MK"><speaker>MK</speaker><p>	Yes, that’s all I can say to that. Thank you very much for that. We’ll come back to that a little bit later. I briefly wanted to touch upon the History Today, Trustees Award that you received in 2011 with Bob Shoemaker. I’d like to ask you in the context of that, what role do you think digital resources have to play in the development of present and indeed future histories? </p></sp><sp who="#TH"><speaker>TH</speaker><p>	That award was really a recognition of the work Bob Shoemaker and I had done over the preceding twelve years, in digitising large volumes of primary sources, and making them useful to historians in particular. It was a recognition of our role in creating the infrastructure that has ensured that all historians are now digital-historians. Because of our work, and that of thousands of other historians, archivists and librarians' archives (and commercial companies), the nature of the library and archive have simply changed, and with it how historians approach research.  Even if you say, ‘right okay, today we’re all off to the archive and we will get our hands dirty handling manuscripts and dive deep into the well of physical remains of the past’, every historian now takes along their phone or camera and takes hundreds of images, to be read, digested and organised at home on a computer. And the images that result, and the data historians now work with, are just as much digital simulacra, mediated representations of material objects, as the archives that have been professionally digitised, and accessed online. The process is simply not the same as sitting down twenty or thirty years ago, a pencil in hand, taking notes as you read through a physical manuscript from cover to cover.  The evidence of the scale of the change that strikes me as most telling, is that when, after the pandemic, the National Archives in the UK re-opened, they literally forbade the use of pencils and pads, and mandated the universal use of cameras. You are heavily discouraged from sitting down, reading a source and taking notes.  </p><p>  For me, the award in 2011 marked the end, or perhaps a high point, in the digitization of the Western Print Archive, second edition, and I was very privileged to have played a role.  It was towards the end of a fifteen to twenty years project, in which everyone pretty much agreed that we needed to digitise everything possible.  And by 2010 pretty much every word printed in English from Caxton to 1923, when copyright got in the way, was available for keyword searching or at least reading online.  To someone trained in a research library in the 1970s, it was incredible. Of course, it also marked a rather darker transition.  In a British context it was about this time that the AHRC (Arts and Humanities Research Council), the main funder for a lot of that work, essentially decided that they were going to abandon digitisation and the publication of scholarly editions online and leave that to commercial publishers like ProQuest and FindMyPast. The AHRC simply exited the field, leaving a lot of university based digital groups struggling. For me, the direction of travel from that point is exemplified DC Thomson &amp; Co Ltd’s purchase, via its subsidiary BrightSolid, of FindMyPast and then Friends Reunited Ltd and its subsidiary Genes Reunited Ltd.  This was just one of several large media conglomerates that expanded dramatically into both family history and academic journal publishing.  These are multi-billion-pound corporations, and they have been allowed (and encouraged by government) to lay claim to intellectual property of a sort that up until this time had been largely the managed and preserved by University and GLAM based academics, librarians and archivists.  In the process they have created a kind of rentier economy, that will allow them to make profits from public documents into perpetuity.  Arguably this transition was just an extension of what we had all seen happen in the music industry in the 1990s, and with Google books in the 2000s, where digitisation led to dramatic disruption in major parts of the economy.  Nevertheless, for me it came as a profoundly disheartening turn of events. </p><p>  On a more upbeat note, I also think that there was something else really interesting going on at that moment in the background, which I hope will create opportunities for historians in the future.  I don’t know if I’m talking to the point here, but one of the things I’ve been struck by, particularly since 2013 or so, is the rise of persistent identifiers (PIDs), like DOI’s and dozens of similar systems – most recently IIIFs.  Of course, persistent identifiers have been around since the rise of the ISBN in the 1960’s and were from the beginning designed to tie down bits of IP, of text, of information, of data, and make it readily findable, which in turn, would allow it to be endlessly monetised. But in the early 2010s with all the enthusiasm around the rise of Blockchain technology in relation to the heritage industry in particular, PIDs seemed to take on a new significance. And what that does to historians is interesting because it begins to create the possibility of bringing all the materials around particular subject – whether it is a person, a war, a date or a place – into a collection of materials, perhaps generated as a series of non-fungible tokens. In part this is what we already do when we collect research data. But, with PIDs for every archival object, you could relate each item to its fullest context – including its relationship in an archival collection to every other item in that collection.   In the process the ‘object of study’ becomes something quite new. First of all, it becomes essentially everything associated with that page or that quote in the hierarchy of the archival descriptions, ensuring that every item is given a larger context.  And second, it allows you to zoom in and zoom out from a main collection to some or all of the associated materials.  The distinctiveness of any single item becomes immediately testable against its fullest context. Of course, this is the upbeat interpretation. In line with my comments about DC Thomson et al, one also has to recognise that at least in part, what is driving these developments and creating this possibility is the same old desire to tie down and monetise public data.  It is also true that the STEM side of the academy has been making all the running in this area, in relation to academic library and data management practice, and that as a result historians will end up using methodologies and practices that were designed for somebody else – and therefore freighted with unforeseen impacts. By way of an example, one of the tools I am having a good time thinking with at the moment is called Zooniverse. </p></sp><sp who="#MK"><speaker>MK</speaker><p>	Can you describe it briefly?</p></sp><sp who="#TH"><speaker>TH</speaker><p>	It’s a citizen science site which allows people to do undertake data entry and data cleaning. It’s been used in projects in geography, biological sciences and astronomy – among a lot else.  It was set up entirely for STEM use and yet in the last three years more and more humanities projects have started to use it.  Something like two thirds of the projects Zooniverse now lists are humanities and historical projects. And then you look at something like Zenodo, I don’t know if you come across that? </p></sp><sp who="#MK"><speaker>MK</speaker><p>	Yes.</p></sp><sp who="#TH"><speaker>TH</speaker><p>	Like Zooniverse, Zenodo was set up by and for STEM. A brilliant data repository, based out of CERN, funded by the EU, it is evolving to become the normal place where digital history project deposit and preserve their data.  And the only real point is that facilities like Zooniverse and Zenodo were created with the priorities of STEM subjects in mind and using will change how historians work.   </p><p>MK	I think that’s a very important point to see where those tools come from and what disciplines lead the way and what are basically left to scout to field and use what they might find for themselves that affects your practise very much so. </p></sp><sp who="#TH"><speaker>TH</speaker><p>	With few exceptions, and one might point to corpus linguistics, the humanities just haven’t been very effective in creating tools of our own. </p></sp><sp who="#MK"><speaker>MK</speaker><p>	Oh yeah, that that’s an ongoing problem in DH that you deal with tools that were never really designed for what you do with them, and they were designed by people who do completely different things and had very different things in mind when they developed that tool. </p></sp><sp who="#TH"><speaker>TH</speaker><p>	At the moment I’m quite interested in the way in which the rise of data science as a discipline area is having an effect on that. The example that springs to mind is the Turing Institute. Set up in 2015 and funded by the UK government, it is big and very flashy. They have three different types of coffee maker in the entrance hall and there is a bit of a ‘tech-bro’ feel to it all. The idea is that they are creating a cadre of academics who are domain experts in data science, who are then supported to collaborate with domain specialists from other disciplines.  It is a very different model of how most projects in the digital humanities have worked in the past, in which you frequently end up with some historian trying to get some computer scientists to make a website for them. The nice thing about the Turing is that they are creating a career structure and a system for data science that is not domain specific in terms of where the data comes from, but which lays the foundation for a more equal partnership between data scientists and, for example, historians. It forms a development that I believe is going to have a significant impact in the medium term on digital scholarly editions, and the whole issue of who gets funded, and what sorts of outputs they are funded to produce. </p></sp><sp who="#MK"><speaker>MK</speaker><p>	Yeah, maybe, because we touched on it, if I quickly cycle back to all the works in the projects that you have been involved with, so, such as The Old Bailey Online, London Lives, Locating London’s Past and The Digital Panopticon, kind of touching upon what we just discussed; I would like to know what’s the common motivation behind all those projects? </p></sp><sp who="#TH"><speaker>TH</speaker><p>	To answer this, you really have to go back to the 1990s. It was clear in the first few years after the rise of the internet, particularly from about 1995, that there was a new way of making historical materials available ‘online’. By mid-90s people had begun to secure funding for digitisation projects. But in my view, they just weren’t very good, and more importantly, they were focussed on elite materials. So you had projects on the letters of philosophers in the 18th century enlightenment, or ‘important’ politicians, but little that reflected the lives of working people.  I write ‘history from below’ in the British Marxist tradition, and one of the great sources for that kind of socio-cultural history was the Proceedings of the Old Bailey, which created a consistent run of published trial accounts, including supposedly verbatim trial materials from 1674 to 1913. It contains 127 million words of text reflecting about 200,000 trials. As a printed series (or microfilm) it was one of those sources that nobody could ever read in its entirety.  A problem made all the more difficult because there was no index or effective way of navigating the 127,000,000 words.  Everybody knew it was full of fantastic stuff but that it was almost impossible to use. It struck Bob Shoemaker and I that it was a perfect candidate for digitisation, but in retrospect, what was really interesting was the timing. We started planning and applying for funding in 1998, the same year that the first version of XML was published.  On the advice of Michael Pidd at Sheffield we became early adopters, and the Old Bailey Online became one of the first big implementations of XML in relation to historical documents. We also adopted a ‘double entry re-keying’ process, sending the images of the original to India to be transcribed twice, and the versions compared to identify errors.  We chose rekeying because OCR was not good enough to work with 18th century text at the time, and as a result the transcription was 99.9% accurate.  This was possible, again, because of timing.  In the previous forty years India had witnessed the development of a series of ‘re-keying agencies’, primarily to service Western insurance companies and other corporations, who had spent much of the ‘70’s and ‘80’s shifting large systems from paper to digital formats. By the 90s, that work had dried up and they were really looking for alternative forms of employment and work and so those two things came together. We ended up with a system the both ensured perfect transcription - because OCR wasn’t good enough to work with 18th century text at the time – and XML mark-up that allowed us to combine structured search and keyword searching, with images of the original, in ways that nobody had seen at the time.  And finally, because the Proceedings had been microfilmed, it was possible to generate images of the original for pennies a page, when working from hard copy would have increased the cost tenfold.  I remain dumbfounded by how it all came together, and how since its launch in 2003, it has created a new body of scholarship.   </p><p>  	London’s Lives was really an extension of these same approaches into manuscript material. Then came Locating London’s Past from around 2008, which was a reaction to the realisation that the technology underpinning Google Earth was available for reuse. Then, Connected Histories a year or two later, and finally.  Looking back, the fifteen years between 1998 and 2013, saw a whole new world of tools and approaches being created, and we pursued as many as we could. As well as these three, there were a bunch of other projects and proposals developing software and trying to tie together different types of sources – all responding to a rapidly evolving conversation. </p><p>  Then, in 2013 or so, Bob Shoemaker started a collaboration that would lead to the The Digital Panopticon, and I ended up going into that. At the time I was chair of the AHRC’s Digital Transformation Working Group, who funded the project, so the conflict of interest, meant that I couldn’t participate in the development of that grant – something that I regret.  What The Digital Panopticon did was to take a lot of the technology and approaches we had developed for London Lives and The Old Bailey Online, and repurpose it, working with 19th century manuscript material related to Australia and the Criminal Transportation. Because I wasn’t involved in the design, I felt the project missed out on some important intellectual issues.  But more importantly, that project also pointed out to me, that what I always thought of as history from below and working-class history in a British context, frequently read as forms of white nationalism and settler nationalism in an Australian context. So having a criminal ancestor is really a claim to whiteness. </p><p>  Since the Digital Panopticon my projects have taken a different direction. One of them has been drawn me into supporting a lot of work in audio and video materials.  We recently launched a website of the oral histories of the BBC – The Connected Histories of the BBC. And it is a direction I very much like.  What became increasingly apparent to me across the 2010s, is the extent to which historian’s and humanist’s obsession with text is misguided. All data is data and one of the huge affordances of the online and digital is that sound is a data stream just as much as text or image. All of them are data streams that simply need to be coordinated, so restricting oneself to text alone feels increasingly nonsensical. As a result, these last few years have been occupied with image and sound and 3D.  We have been modelling historical spaces and soundscapes, working with speech to text methods.  This has involved a collaboration with Ben Jackson at the University of Sussex that has been hugely productive.  Another side of this, has involved building tools for data capture, so, for instance, making holders for securing your phone in an archive and an app to facilitate image management – creating all the appropriate metadata along the way.  In some respects, this work just circles back to the issue of PIDs and findability. </p></sp><sp who="#MK"><speaker>MK</speaker><p>	Thank you. Maybe just coming back to the practical aspects because you already mentioned the transcription work, just generally speaking, how did you achieve this huge output, you mentioned the transcription, so was all transcription done by hand or generally what barriers needed to be overcome for these projects?</p></sp><sp who="#TH"><speaker>TH</speaker><p>	Early on there was a lot of happenstance and real luck. At the time, in 1998, I was working for the University of Hertfordshire which was host to an organisation called the Higher Education Digitisation Service which where Simon Tanner, who is now at Kings, undertook project work.  He organised the photography for the microfilm and the management of the re-keying in India, delivering both of those to Sheffield as a kind of sub package which I was nominally in charge of. In other words, Simon very much created one half of a publication pipeline. And while it was complex, it came together remarkably easily, much more by happenstance than by design. We then got everything to Sheffield and what is now the Digital Humanities Institute.  That end of the process was initially less well understood.  There were just a lot of people involved, but nobody who really had experience of doing this kind of stuff at that scale. Most of our experience up to then was in creating small CD-ROM projects. My recollection is that it took three or four years for the Sheffield team to really build up the skills needed to take on new projects at this scale.  And that we didn’t get it all ironed out, until Sharon Howard joined the team at the beginning of the London Lives project. Sharon has been the data wrangler and project manager for not just our projects but probably half the large-scale digital projects in the UK. The other central figure was Jamie McLoughlin who is the web developer, and who has been continually upgrading and refining the sites for the last twenty years. But to answer the question, once the teams and processes were in place - re-keying, images draw from microfilm, XML mark-up – we had a remarkably efficient development pipeline that ensured the projects kept coming, and the outputs kept getting made. </p></sp><sp who="#MK"><speaker>MK</speaker><p>	Yeah, ok, very nice. Thank you. Just to follow up on that, is there something that you’d see as essential elements of these resources that you created or maybe to put it differently, is there advice that you’d have for someone thinking of creating a similar resource?</p></sp><sp who="#TH"><speaker>TH</speaker><p>	Know why you are doing it! Let me do this by way of an anecdote. In about ‘98/’99 there was a £50 million funding programme called the New Opportunities Fund that was part of the Heritage Lottery Fund in the UK, and from 1999 to 2002 or so they funded a hundred and twenty different projects, mainly located in local authorities, but also in in the Academy. And they held everybody’s hand and they forced everybody to do huge amounts of planning and due diligence as part of the grants programme. The application that Bob Shoemaker and I put in ran to two volumes and I think it was 170 pages in total. About ten years later, David Thomas, who was then director of Digital at the National Archives, did a survey of all those sites and projects, and within a decade two-thirds of them had disappeared or had simply not had any development since the end of the funding, and no evidence of reuse or community building. And that’s kind of the huge downside in all of this. If you don’t have a community that wants this material, if you don’t have some strategy for institutional adoption of it, it’ll have a nice launch party, run for six months, and fall over. And that’s by way of offering the advice that you’ve work at thinking through your audience, and some kind of sustainability strategy. And I would also say that the art of it is building a community as you go and making sure that you are consulting and dealing with people who want to use what you’re making. </p></sp><sp who="#MK"><speaker>MK</speaker><p>	Thank you and then lastly, if you were to start any of these projects again, what would you do differently? </p></sp><sp who="#TH"><speaker>TH</speaker><p>	I don’t know that I would do anything differently. My feeling is that all of these projects are a response to their particular historical moment. They were a reaction to an ongoing dialogue in the digital humanities, and history more broadly. Obviously, I’ve told you how I feel about The Digital Panopticon, I would think differently about that. And I would also probably have moved out of history more fully than I have. If the point is to make positive change, I think there was a real opportunity for me to help shape UK and European policy in the 2010s, and I didn’t pursue that as fully as I should have done.  </p></sp><sp who="#MK"><speaker>MK</speaker><p>	Thank you Tim. Thank you so much. That was very interesting to hear. Just before I end the recording, is there anything else that you feel like we should talk about, and we haven’t? </p></sp><sp who="#TH"><speaker>TH</speaker><p>	Just in terms of digital scholarly editing it strikes me that there’s more to be said about the technologies that are being used and how the people involved are either trained or not trained in using them.  The technologies we use put real limits on the choices we make. So, the minute you choose one technology or tool over the other you are caught and unless you engage deeply with the process, you simply won’t be able to predict the outcome. I believe more needs to be done to train undergraduates and post-graduates, and what I am hoping will emerge in the medium term is a toolkit that allows us to more precisely measure the limitations of the approaches we adopt. </p></sp></body></text></TEI>