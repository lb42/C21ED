<?xml version="1.0" encoding="UTF-8"?><TEI xmlns="http://www.tei-c.org/ns/1.0"><teiHeader><fileDesc><titleStmt><title>C21 Editions:  Joris van Zundert</title><title type="sub">Interview conducted by  James O’Sullivan		 on January 31st, 2022</title></titleStmt><publicationStmt><p>Unpublished working draft</p></publicationStmt><sourceDesc><bibl> O'Sullivan, J., M. Pidd, M. Kurzmeier, O. Murphy, and B. Wessels.
                            (2023). <title>Interviews about the future of scholarly digital editions
                                [Data files].</title> Available from <ref target="https://www.dhi.ac.uk/data/c21editions">https://www.dhi.ac.uk/data/c21editions</ref> (File C21_JOS_JVZ.xml downloaded: 8th June 2023).</bibl></sourceDesc></fileDesc><encodingDesc><p>Retagged in TEI P5 from RTF (via soffice and docxtotei) </p></encodingDesc><revisionDesc><listChange><change><date>$today</date>Header added</change></listChange></revisionDesc></teiHeader><text><front><div><head>Interviewee bio</head><p>JORIS J. VAN ZUNDERT is a senior researcher and developer in humanities computing in the department of literary studies and the Digital Humanities Lab at the Huygens Institute. His research focuses on computational algorithms to analyze literary and historical texts, and on aspects of humanities information and data modeling. His PhD research in the field of Science and Technology Studies focused on methodological effects of the interaction between software engineers and humanities scholars. He was awarded the title of doctor “cum laude” (with honors) on the basis of his dissertation Scholarship in Interaction in 2022. His computational analytic work focuses on the correlation between text immanent features of texts and sociological processes around the concept of literature. He is also involved in developing computational approaches to stemmatology, narratology, and scholarly editions.</p></div></front><body><sp who="#JOS"><speaker>JOS</speaker><p>	What do the terms scholarly edition and scholarly editing mean to you?</p></sp><sp who="#JVZ"><speaker>JVZ</speaker><p>	That’s a hard question actually, well scholarly editing in any case is the academic labour to produce a text or a representation of a historical text I would say –or pretty much actually any text but usually historical text– to either study that text as a scholar or to represent a particular adaptation of that text for a particular audience. I think that’s the most shorthand definition you can get of scholarly editing. And then an edition; yeah that’s really a good one, I guess most of our peers regard a traditional or print edition or a digital edition the representation of a particular text in print or digital fashion. Personally, almost anything can either be the object of editing or the representation of the object that is edited, these days in any case. I’ve been discussing a lot with other scholars in the field, for example, can you edit a building, or can a 3D model or a simulation be regarded as an edition as well? And we seem to agree that, yes under particular constraints, why wouldn’t it count as a work of editing. It all depends on what you’re editing, are you editing a building, are you editing a stage play, are you editing a poem. So, in the end I think it all generally boils down to: do you want to represent some thing, some object that has a particular cultural or heritage value and do you want to create a representation of that in whatever shape, way or form. And the scholarly bit comes in when you do that in a sort of verifiable accountable way, I would say, because, you know, anybody can reproduce a text, I guess, but it comes down to how you account for how you do that and what decisions you make whether they become scholarly or academic or scientific. Pick your poison, I guess.</p></sp><sp who="#JOS"><speaker>JOS</speaker><p>	What do you feel digital editions do well and what might they do better?</p></sp><sp who="#JVZ"><speaker>JVZ</speaker><p>	Well, certainly what they do better in any case almost even without paying attention to it, is accessibility. I mean, it’s clear that I don’t need to travel half the globe to get to certain editions these days and also you know you get goodies for free usually like full text search and downloadability. So yeah, I guess availability. I would say, they two-fold, triple fold scale that marvellously well, of course, digital editions. But that’s almost low hanging fruit, you know, it’s more the result of the internet being there, I think, than that our ways of creating or doing or making scholarly editions changed. So, what I would hope how digital editions develop is that they become much more multimedial. Why would we constraint ourselves to the limits that we are used to in print form, the text on the page and maybe a picture or two. Why wouldn’t we use any material, indeed 3-D models or simulations or moving pictures, spoken text, music? Really anything that either particularly pertains to the content of that what you are editing or that what could be used to represent it better, or more accessible, or more explained. I would want digital scholarly editions to use all those materials.</p></sp><sp who="#JOS"><speaker>JOS</speaker><p>	That leads nicely into your essay ‘Barely Beyond the Book?’, where you call on theorists and practitioners to intensify the methodological discourse necessary to implement a new form of hypertext and edition that truly represents textual fluidity and text relations. Without doing so, we’re stuck with digital copies of print, or as you put it, we will limit the digital scholarly edition to the expressiveness of print. Do you still feel this way?</p></sp><sp who="#JVZ"><speaker>JVZ</speaker><p>	Yes most certainly actually, yeah it’s… As a field we are learning of course, we hardly know where digital scholarly editions can go. And, while we have ideas and there is some experimentation, as long as we keep ourselves constrained to the print paradigm kind of rules of the game, as it were, we won’t have much additional scholarly advantages, I think, besides the advantages and the benefits of availability that we just talked about. Apart from that we won’t progress anything about our process, about our methods by using these digital or computational means. So, you know, I stand still firmly behind what I developed as a theory in that piece, yes.</p></sp><sp who="#JOS"><speaker>JOS</speaker><p>	You’ve also described large scale digital infrastructures as a dead end for digital humanities.</p></sp><sp who="#JVZ"><speaker>JVZ</speaker><p>	Ah yeah.</p></sp><sp who="#JOS"><speaker>JOS</speaker><p>	I wonder if you could expand on that position?</p></sp><sp who="#JVZ"><speaker>JVZ</speaker><p>	Yeah, that was a controversial one, that was a good one (sorry for patting myself on the back there). Yeah, the problem is not that I don’t believe in large infrastructure. I mean, it’s wonderful if there is large infrastructure that scales and so that you can expand your work or scale the process of your work or you can scale access or you name it what you would want it to scale. The trouble with the large scale infrastructure projects that we have seen until now in the humanities, or which I at least have seen in the humanities (that’s quite a lot since 2005), is that they regard infrastructure itself as an aim, as a goal, an objective by itself. And what you get is that the infrastructural technology becomes leading and this is where you lose the –what in my view would be the paramount thing to do– the ability or the objective of supporting the research that people are doing or the editing that they are doing. So, what I have seen is that… large digital infrastructure projects became much too much involved with being infrastructure to be able to support specific research. So, what I tend to say is that infrastructure follows innovation. It should follow the scholarly work that is being done. Building infrastructure the other way around, so rather a priori imagining what not yet existing infrastructure would be needed, and then insisting that any scholarship will be done on that infrastructure… I’ve never seen that work. And this reasoning comes from a very simple method or process that is used in software development which is where you say, you know, do it in an iterative way or do it in an inductive way. So, you take one small problem, you solve that and if you see that your particular solution works, you try the solution on another example of the problem. If that works well too, you have reason to say that maybe you have a solution that works for more examples. And inductively you try to scale that solution. And if you scale it far enough by adding examples then eventually by itself it becomes infrastructure because you build something that works for an awful lot of instances of the same problem. But again, the other way around, you know, thinking that you can sort of parachute a complete digital infrastructure that solves all the problems that you had imagined could exist in a scholarly process or even in a scholarly domain, I think you set yourself up for failure.</p></sp><sp who="#JOS"><speaker>JOS</speaker><p>	What are the dominant infrastructures and platforms that come to mind when you think of digital scholarly editing?</p></sp><sp who="#JVZ"><speaker>JVZ</speaker><p>	Are there any? You know I… infrastructure or platforms that are in actual broader use? Maybe EVT? The edition visualisation technology, that’s a pretty thin platform I would say. I don’t mean that in a negative way, like it doesn’t have too much to offer. I think it is actually a sensible choice because it is a choice to do only the visualisation and they do that rather well. From where I’m sitting, I mean TextGrid in the German context, I think they self-concluded that it failed. I don’t hear anything about it anymore. There’s supposed to be digital infrastructure for the humanities in general in the DARIAH context of course, but essentially a lot of the national level DARIAH projects never went anywhere. There was a bit of data access but never anything really substantial as to supporting digital scholarly editions, as far as I know. We ourselves at the Huygens Institute we still maintain a tool called eLaborate which is a platform that is specifically geared towards creating and hosting digital editions that’s leading a marginal existence I would say. It is used, I would say, but it’s not particularly loved by scholarly editors I have to admit. I’m sure I’m forgetting a whole lot but there’s no… as far as I can tell there’s no really successful large-scale platform or infrastructure specifically geared towards scholarly editing. Of course, there’s the work of Peter Robinson in the Textual Communities project, but did that find much uptake? And CTE, the Classical Text Editor might be mentioned, a kind of Word for classicists, not ideal but still used. </p></sp><sp who="#JOS"><speaker>JOS</speaker><p>	What about the TEI Guidelines?</p></sp><sp who="#JVZ"><speaker>JVZ</speaker><p>	TEI is of course the major successful maybe even the only successful de facto standard for structuring text in the domain, but it’s not a platform, it’s not an infrastructure, I would say. There are infrastructural aspects to TEI: TEI Publisher and there’s the website of the TEI Consortium. But the TEI is much more a community of scholarly editors who think about what would be useful taggings or annotations, or annotation structures for digital scholarly editions or for editions of any kind really. You could call that an infrastructure of knowledge, but it’s not really a technical infrastructure. The technical infrastructure they use is the Internet and a generalized markup language XML. Both technologies not specifically built for scholarly editors, but successfully adopted by them. And they have been majorly successful. In such a way that they have also been too successful, I think, because, well many people also think TEI-XML is the only thing you should use when creating digital editions. Which again introduces the problem of how many constraints from a specific paradigm you want to have in place or want to put yourself under when you are creating a digital or computational edition. Because you also want some kind of liberty of thinking how you want to structure or format or develop your scholarly edition.</p></sp><sp who="#JOS"><speaker>JOS</speaker><p>	You mention international projects like DARIAH; why do you think there has been a lack of success at the national level in terms of actually producing and supporting activities like digital scholarly editing?</p></sp><sp who="#JVZ"><speaker>JVZ</speaker><p>	Well, many reasons. I can only really speak for the Dutch situation, I think, but I guess it sort of applies in other places as well. DARIAH in the Netherlands is very much a follow up or a merger with the CLARIN initiative and CLARIN has always been very strongly tied to linguistics kinds of research and scholarship. And there’s nothing wrong with that of course, but computational linguists are not the best scholarly editors maybe. Their focus is really on text streams as well as annotation of language and grammar. They are really into building parsers, building and evaluating linguistic analysis tools for whatever data the parsers parse texts into. So, there’s not a lot of scholarly editing going on in there. I think the CLARIN-DARIAH initiative was quite poorly informed about what textual scholars think the scholarly process is that is tied to creating print or digital editions. And on the other hand, textual scholars see themselves as lecturers or people studying historic texts, and as people creating representations or digital editions of historic texts, not as linguists or technologists in any case. I think also they were not well versed enough in digital or computational technology to really express to the people doing the building and developing what would be useful to them or what would be usable by them. So yeah, there seems to have been lots of opportunities for miscommunications and misunderstandings about how one would tie existing scholarly processes to particular digital technology. So yeah, eventually I think it boils down to a lot of misunderstanding on the side of the technologists what would be useful and usable for scholarly editors, and on the other side a lot of misunderstanding and a lot of lack of knowledge of how digital technology might support your scholarly work and process.</p></sp><sp who="#JOS"><speaker>JOS</speaker><p>	I wonder if we could change topics slightly to your idea of scholarly editing and the creation of software as forms of revisionary authorship; could speak to that idea?</p></sp><sp who="#JVZ"><speaker>JVZ</speaker><p>	Yeah so, although in the development community for research software engineers this is still quite a controversial point, I would say that writing code is also a form of authorship. Whether we like it or not, we do produce symbols or signs that are somehow ruled by syntactical constraints so that software can be produced. So, I like to think about that as authoring and not so much as engineering or developing. And if you start applying that kind of authoring, so a different set of symbols to the same scholarly aim, namely producing representations of historical text, then in my view that counts as revisionary authorship. You’re just, you know, you take a text that’s in existence or an object that’s in existence, made by somebody before you, usually an author, and you use it as data that you process and transform into something else. And whether you like it or not, simply because it becomes something else you are involved with that process as an author of a certain kind. You cannot … What a lot of scholarly editors but also very much a lot of developers and programmers want you to believe, is that their process, that their involvement with creating the transformation. that all that is completely neutral. They want you to believe that the effect of the transformation is not up to them. That it is not them taking the decisions, it’s not them changing the historical information into the other representation. But really, if you look at it, that’s nonsense because if they wouldn’t do that process, if they wouldn’t take the decisions, that whole transformation would not happen. So by merely doing their job they already are, for a matter of fact, making a new and another representation of the source that they are editing or that they are programming. Or they are programming the transformational tools for that. And then by definition, I think, that means that you are doing revisionary authorship. And the point for me was not so much to define that process as revisionary authorship. My point was more to try to get them aware that you can view your work as revisionary authorship and then to ask what does that mean? Does that change the way you think about accountability? Does that influence the ethics of your work? Does it mean that you start thinking more about the product that you are developing or producing? So, for me it was really sort of an intervention to get them thinking harder about what their involvement with these historical sources means, and what responsibilities come with that. And I meant it also in that sense as a positive push. I heard that some developers have interpreted my writings as sort of pushing them in a corner to sort of come clean and admit that they should be more accountable, more responsible in formal ways for their work. But that’s not essentially what I meant. What I meant was, that if you look at your work as revisionary authorship, you probably start thinking more along the lines of “hey I’m also involved with this actual source, with this actual text that I’m developing the tools for”, And for me that meant that developers should be credited more for that work because they very rarely actually are. I mean, there might be some kind of a gratuitous acknowledgement of the developers, but I think they are really doing invaluable scholarly work. So, they should be completely involved and credited for that. They should also be accountable for it. And, I think, they should be thinking a lot more about what their work means within that domain.</p></sp><sp who="#JOS"><speaker>JOS</speaker><p>	Are there issues of invisible labour across digital scholarly editing, in general?</p></sp><sp who="#JVZ"><speaker>JVZ</speaker><p>	Yeah most certainly, I mean yeah, well ok this is a hard one because I am very much influenced and biased by my personal context and I’m working a lot with these guys and girls that do compute, that do code, but are also aware of how their context is very scholarly and they are doing scholarly work. And I think that’s really, that’s not the normal situation as it were. The normal situation, I think, at the moment is still like ten years to go: that there will be this scholar or professor that brings… he or she is doing all this scholarly work and outsources this, in her view more material labour, to developers and engineers, so she can sit back and admire the end product. So, there’s really still a very sharp division to what is the scholarly intellectual bit and the kind of production level labour bit to create a digital artifact from all that intellectual knowledge. Whereas I think that this separation is something of a fallacy in academia. Essentially the work needs to go together to produce a valuable and valid scholarly product or output and I still think it’s the case that that kind of work, especially engineering work, is… Yes, people know it happens but people are not too much interested in what the scholarly value of that is, and that it should be rewarded. And indeed it should be much more visible, just because it’s such an important part of the scholarly process these days.</p></sp><sp who="#JOS"><speaker>JOS</speaker><p>	You’ve published quite a bit on the topic of interfaces to digital scholarly editions, I wonder if you could speak to that matter?</p></sp><sp who="#JVZ"><speaker>JVZ</speaker><p>	Yeah, interface is interpretation, isn’t it? There is simply no way, like there is no neutral process to transform a particular historical source into a particular digital computational representation, there is no way I think you can create a user interface that does not somehow affect the meaning and certainly the possible interpretations of the historical source that you’re editing. Suppose that I would create a TEI data set for a particular text and I only disclose this text via some kind of online tool. Then that says something about who I want to support in being able to interpret the text at all, but it also limits very much the possible representations of what I edited. On the complete other side of the spectrum of course there is something that you… well I don’t know… maybe you could 3-D model a complete facsimile for example. And I cannot tell you much about how this changes interpretations of the historical sources in particular, but it’s clear from the example that the space for interpretation is very much defined by those kinds of differences between the interfaces. So it’s not… I think a lot of scholars still tend to think that as long as the data exists in some kind of XML form, or any form that’s at least more or less machine readable and somewhat sustainable, that the scholarly work is done. But essentially that’s only half the work because you have to provide also some kind of possibility for the audience that you imagine to engage with your edition and the only thing we have as a tool for that is building some kind of interface. Now you could say that “No, I’m only serving… I’m only intending to serve scholars that are able to reuse the TEI XML source.” Okay, that’s fine, that can be an audience that you define and that you cater to in this way. But any other kind of definition of your audience will require a particular interface. And essentially, I think there has been way too little debate and way too little experimenting with how different interfaces cater to different audiences. If you take ten samples of what would count as… well I don’t want to say middle of the road… particularly well done scholarly digital editions actually, you will get ten times the same impression. It’s always this thing of “there will be a statement of where the TEI XML sources are and you have this screen with the facsimile, and this one with the transcriptions and annotations”. So really there’s been, over these last two decades, this kind of consensus about the model for the interface of a scholarly digital edition. And that is great in one sense: apparently there is something of a consensus on how that should minimally look and how it should minimally be useful. But it’s also not too creative, not too imaginative. There are probably way more interesting ways in which scholarly editions could be interfaced or presented. There is this bland, pretty obvious kind of access, I would say, but there’s hardly any experimentation with other possible interfaces.</p></sp><sp who="#JOS"><speaker>JOS</speaker><p>	And what about the role of computer assisted machine learning in the future? I know that you have experimented with computer-assisted collation in the Beckett project, do you think automation will play a bigger role in scholarly editing in the future?</p></sp><sp who="#JVZ"><speaker>JVZ</speaker><p>	Absolutely yeah, it will take a long time still I think because we have, certainly in a Dutch context, we have problems of education, we’re not training the next generation of scholarly editors or scholars in general actually too well, we don’t set them up too well to use computational means and tools. But yeah, simply by taking the formidable power of deep learning methods and techniques, and AI techniques you will be able to do much more interesting things with your digital text than before. So there… You know, I can imagine that we… You know, there will always be a place for the good old-fashioned handwork like going to a library and typing your keywords into the catalogue, seeing what turns up, reading those books and synthesizing all the knowledge you gather that way. But I think more and more that kind of scholarly process will be supported semi-intelligent language tools and information tools. At some point pretty much every scrap of secondary literature will be digitised. I mean Google Books has almost already digitised all books, although they don’t disclose all those texts in a computational useful way for scholars, of course. But that means that if all that information is eventually available as electronic text, why would you go hunt by library catalogue and your brains only? Why wouldn’t we use information discovery tools and deep learning tools that can learn from our queries what particular topics we’re interested in? That will get more interesting information for us and will even pre-synthesize it for us, sources listed and all. I can well imagine that the scholarly bit of annotation to provide glosses or translations or definitions of certain words will not be a manual job soon. I think at some point there will be so many high quality historical electronic dictionaries that, you know, that process is no longer needed to be done anymore by hand. It’s merely programming a small tool or a small module inside of your digital edition that will query the right information, right lexicographical, etymological information from the right database to provide those glosses for your readers. I mean, that is pretty much the technology that is already available, it’s just that we don’t have a sufficient sophisticated integration of these technologies to make this a reality already. But once this is possible say in five years or even sooner, would you really, as a scholar, still want to be doing that work manually? Again, there will always be these tiny corner cases that will require this kind of manual labour. But in all other cases I think you want to use your time for more challenging aims in that case. So yes, definitely we are still figuring out how we can use all these nice shiny new computational tools, but yes they will definitely have a big impact on scholarly editing. And on scholarly research of any kind basically.</p></sp><sp who="#JOS"><speaker>JOS</speaker><p>	So finally, then, if I gave you unlimited resources, a sort of magic wand scenario, what would the digital edition of the future look like to you?</p></sp><sp who="#JVZ"><speaker>JVZ</speaker><p>	Unlimited time resources… I think this is not even dreaming beyond the next five years, I guess, but I think in general the process will look like this. You make a transcription using HTR –so Handwritten Text Recognition– using something like Transkribus or one of its open source friends. You will then not start creating normalisations of the language or the transcription by hand, but you will program some transformers that will regularise or normalise the text as far as it is needed for your particular edition. You will then program or pull together the scripts that already exist to do the annotation on those textual sources and then you will probably publish your edition in a pretty much standardised digital repository. Or you may want to create some kind of showcase scholarly edition out of that source, and you will start programming a rather more sophisticated representation for your particular edition, drawing in more digital resources as you need. In all I think that means that the scholarly editor of the future will much more be able… he or she will not be a programmer, that’s not what I’m saying, but he or she will be much more involved in writing short scripts or small programs or small codes to support him or herself in that scholarly process. So, it will involve much more coding practices, I think. But if you look even more in the future… I’m pretty sure this will happen, this is not even something that we will be able to gatekeep out of scholarly practice, it will just come with new generations that are more knowledgeable about these kinds of useful tiny tools, tiny digital tools of computational script and they will use them so it will just flow into the scholarly practice of the future, no doubt about that. Still there will always be people not using those tools and using a print edition, fine with that too. But apart from these developments that will happen, if you really ask me to peer into my crystal ball, well, yeah ehm, will we be at a keyboard still at all? Won’t we be at some kind of other more haptic interface working with complete and tangible 3-D model historical sources. And will we be putting together interpretational tools for ourselves or for our audience in completely new ways we almost cannot imagine yet? I don’t think it will look anything like a laptop or a screen to be honest. It will look much more like a simulated environment, so maybe a… what do you call that… a kind of a 3-D model or haptic virtual interface. What is interesting for me about that, you know, thinking about and speculating about what kind of haptic, really tangible, stimulated interfaces might exist in the future… from an epistemological point of view is that it becomes very interesting what is actually being researched in those environments. Because one thing that seems to be apparent is that it’s not going to be the original historical source, because that will probably be available in a digital form of facsimile that we can’t imagine even yet: a facsimile that even reproduces the material feel, the material existence almost of the historical source, but in a way that we can completely pull apart and inspect and study every tiny bit and aspect of. Without actually needing the original, because, you know, it would be kind of irresponsible to take apart the original. But with a perfect digital representation or simulation we can do whatever we want. I am really curious how that will change how we think about editing and researching historical sources.</p></sp></body></text></TEI>